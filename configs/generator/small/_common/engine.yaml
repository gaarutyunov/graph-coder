engine:
  _target_: catalyst.engines.torch.GPUEngine
  _var_: engine
  gradient_accumulation_steps: 1
  mixed_precision: fp16
  deepspeed_plugin:
    _target_: accelerate.utils.dataclasses.DeepSpeedPlugin
    zero_stage: 3
    offload_optimizer_device: cpu
    offload_param_device: cpu
    zero3_init_flag: true
    zero3_save_16bit_model: false
    gradient_clipping: 1.0
    hf_ds_config:
      train_batch_size: auto
      train_micro_batch_size_per_gpu: auto # `accelerator` configures this parameter from the `batch_size` from the `dataset` config
      zero_optimization: # `accelerator` configures some parameters from the `hidden_size` from the `model` config
        contiguous_gradients: true
        overlap_comm: true
      fp16:
        enabled: true
        loss_scale: 0
        initial_scale_power: 16
        loss_scale_window: 1000
        hysteresis: 2
        min_loss_scale: 1
      activation_checkpointing:
        partition_activations: true
        contiguous_memory_optimization: true
        cpu_checkpointing: true
        number_checkpoints: 1
      optimizer:
        type: Adam
        params: # other params are configures by `accelerator` from the `optimizer` config
          betas: [0.9, 0.999]
          eps: 1e-08
          amsgrad: false
      scheduler:
        type: WarmupLR # other params are configures by `accelerator` from the `optimizer` and `scheduler` configs
