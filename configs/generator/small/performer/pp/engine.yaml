engine:
  _target_: graph_coder.engine.DeepspeedEngine
  _var_: engine
  deepspeed_plugin:
    _target_: accelerate.utils.dataclasses.DeepSpeedPlugin
    hf_ds_config:
      _target_: accelerate.utils.deepspeed.HfDeepSpeedConfig
      config_file_or_dict:
        gradient_accumulation_steps: 1
        train_micro_batch_size_per_gpu: 8
        zero_optimization:
          stage: 1 # pipeline parallelism is not supported for stages 2 and 3
          offload_optimizer:
            device: cpu
          offload_param:
            device: cpu # valid only for stage 3
          contiguous_gradients: true
          overlap_comm: true
          reduce_scatter: true
          round_robin_gradients: true
          allgather_bucket_size: 1e6 # TODO: try to use smaller values for these
          reduce_bucket_size: 1e6
        fp16:
          enabled: false # fast-transformers do not support fp16
        activation_checkpointing:
          partition_activations: true
          contiguous_memory_optimization: true
          cpu_checkpointing: true
          number_checkpoints: 1
        optimizer:
          type: Adam
          params:
            betas: [0.9, 0.999]
            eps: 1e-08
            amsgrad: false
            lr: 0.02
            weight_decay: 0.01
        scheduler:
          type: WarmupLR
          params:
            warmup_min_lr: 0
            warmup_max_lr: 0.02
            warmup_num_steps: 10000
        comms_logger:
          enabled: true
          verbose: true
          prof_all: true
          debug: true