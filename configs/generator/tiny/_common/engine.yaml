engine:
  _target_: catalyst.engines.torch.GPUEngine
  _var_: engine
  deepspeed_plugin:
    _target_: accelerate.utils.dataclasses.DeepSpeedPlugin
    hf_ds_config:
      _target_: accelerate.utils.deepspeed.HfDeepSpeedConfig
      config_file_or_dict:
        train_micro_batch_size_per_gpu: 2
        gradient_accumulation_steps: 1
        zero_optimization:
          stage: 3
          offload_optimizer:
            device: cpu
          offload_param:
            device: cpu
          contiguous_gradients: true
          overlap_comm: true
          allgather_bucket_size: 5e8 # TODO: try to use smaller values for these
          reduce_bucket_size: 5e8
          stage3_max_reuse_distance: 1e9
          stage3_prefetch_bucket_size: 5e8
          stage3_param_persistence_threshold: 1e6
        fp16:
          enabled: true
          auto_cast: true
          loss_scale: 0
          initial_scale_power: 16
          loss_scale_window: 1000
          hysteresis: 2
          min_loss_scale: 1
        activation_checkpointing:
          partition_activations: true
          contiguous_memory_optimization: true
          cpu_checkpointing: true
          number_checkpoints: 1
        optimizer:
          type: Adam
          params:
            betas: [0.9, 0.999]
            eps: 1e-08
            amsgrad: false
            lr: 0.02
            weight_decay: 0.01
        scheduler:
          type: WarmupLR
          params:
            warmup_min_lr: 0
            warmup_max_lr: 0.02
            warmup_num_steps: 10000