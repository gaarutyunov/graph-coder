dataset:
  _target_: graph_coder.datasets.AstDataset
  _var_: dataset
  root: ~/git-py/raw/python
  batch_size: &batch_size 4
  max_length: &max_length 32
  collate_fn:
    _target_: graph_coder.utils.partial
    func:
      _target_: graph_coder.data.collate_ast
    max_length: *max_length
    tokenizer:
      _target_: graph_coder.utils.get_pretrained_tokenizer
      _var_: tokenizer
      name: EleutherAI/gpt-neox-20b
      pad_token_id: &pad_token_id 1
      eos_token_id: &eos_token_id 0
      _mode_: call
    _mode_: call
  random_seed: 42

model:
  _target_: graph_coder.models.GraphCoderGenerator
  _var_: model
  vocab_size: &vocab_size
    _target_: graph_coder.utils.get_vocab_size
    tokenizer:
      _var_: tokenizer
    _mode_: call
  eos_token_id: *eos_token_id
  hidden_size: &hidden_size 64
  embedding:
    _target_: torch.nn.Embedding
    _var_: embedding
    num_embeddings: *vocab_size
    embedding_dim: *hidden_size
    padding_idx: *pad_token_id
  encoder:
    _target_: torch.nn.TransformerEncoder
    _var_: encoder
    encoder_layer:
      _target_: torch.nn.TransformerEncoderLayer
      d_model: *hidden_size
      nhead: 4
      dim_feedforward: 128
      dropout: 0.1
      activation: relu
    num_layers: 6
  graph_encoder:
    _target_: graph_coder.modules.TokenGTEncoder
    embedding:
      _var_: embedding
    lap_node_id: true
    type_id: true
    encoder_embed_dim: *hidden_size
    encoder_ffn_embed_dim: 128
    causal: true
    encoder_layers: 6
    encoder_attention_heads: 4
    performer: true
    attention_dropout: 0
    apply_graphormer_init: true
  decoder:
    _target_: torch.nn.TransformerDecoder
    decoder_layer:
      _target_: torch.nn.TransformerDecoderLayer
      d_model: *hidden_size
      nhead: 4
      dim_feedforward: 128
      dropout: 0.1
      activation: relu
    num_layers: 6

runner:
  _target_: graph_coder.runners.GraphCoderGeneratorRunner
  model:
    _var_: model
  eos_token_id: *eos_token_id
  vocab_size: *vocab_size

run:
  - _call_: train
    logdir: &logdir logs
    num_epochs: 100
    valid_loader: &loader_key valid
    valid_metric: &metric_key loss
    minimize_valid_metric: &minimize true
    verbose: true
    timeit: true

    criterion:
      _target_: torch.nn.CrossEntropyLoss
      ignore_index: *pad_token_id

    optimizer:
      _target_: torch.optim.Adam
      _var_: optimizer
      params:
        _var_: model.parameters
      lr: 0.02

    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      optimizer:
        _var_: optimizer
        patience: 10

    loaders:
      _var_: dataset.loaders

    callbacks:
      - _target_: catalyst.callbacks.checkpoint.CheckpointCallback
        topk: 2
        logdir: *logdir
        loader_key: *loader_key
        metric_key: *metric_key
        minimize: *minimize
      - _target_: catalyst.callbacks.misc.EarlyStoppingCallback
        patience: 20
        loader_key: *loader_key
        metric_key: *metric_key
        minimize: *minimize
