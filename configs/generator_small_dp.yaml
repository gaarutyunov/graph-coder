tokenizer:
  _mode_: call
  _target_: graph_coder.config.F.get_pretrained_tokenizer
  _var_: tokenizer
  eos_token_id: 0
  name: EleutherAI/gpt-neox-20b
  pad_token_id: 1
embedding:
  _target_: graph_coder.modules.TokenEmbedding
  _var_: token_embedding
  embedding:
    _target_: torch.nn.Embedding
    _var_: embedding
    embedding_dim: 64
    num_embeddings: 50277
    padding_idx: 1
  ff:
    _target_: torch.nn.Linear
    in_features: 16
    out_features: 1
graph_encoder:
  _target_: graph_coder.modules.TokenGTEncoder
  _var_: graph_encoder
  apply_graphormer_init: true
  attention_dropout: 0.1
  embedding:
    _var_: token_embedding
  encoder_attention_heads: 4
  encoder_embed_dim: 64
  encoder_ffn_embed_dim: 128
  encoder_layers: 4
  lap_node_id: true
  type_id: true
decoder:
  _target_: torch.nn.TransformerDecoder
  _var_: decoder
  decoder_layer:
    _target_: torch.nn.TransformerDecoderLayer
    activation: relu
    d_model: 64
    dim_feedforward: 512
    dropout: 0.1
    nhead: 16
  num_layers: 8
encoder:
  _target_: torch.nn.TransformerEncoder
  _var_: encoder
  encoder_layer:
    _target_: torch.nn.TransformerEncoderLayer
    activation: relu
    d_model: 64
    dim_feedforward: 512
    dropout: 0.1
    nhead: 16
  num_layers: 8
engine:
  _target_: catalyst.engines.torch.DataParallelEngine
  _var_: engine
  deepspeed_plugin:
    _target_: accelerate.utils.dataclasses.DeepSpeedPlugin
    hf_ds_config:
      _target_: accelerate.utils.deepspeed.HfDeepSpeedConfig
      config_file_or_dict:
        activation_checkpointing:
          contiguous_memory_optimization: true
          cpu_checkpointing: true
          number_checkpoints: 1
          partition_activations: true
        fp16:
          auto_cast: true
          enabled: true
          hysteresis: 2
          initial_scale_power: 16
          loss_scale: 0
          loss_scale_window: 1000
          min_loss_scale: 1
        gradient_accumulation_steps: 1
        optimizer:
          params:
            amsgrad: false
            betas:
            - 0.9
            - 0.999
            eps: 1e-08
            lr: 0.02
            weight_decay: 0.01
          type: Adam
        scheduler:
          params:
            warmup_max_lr: 0.02
            warmup_min_lr: 0
            warmup_num_steps: 10000
          type: WarmupLR
        train_micro_batch_size_per_gpu: 1
        zero_optimization:
          allgather_bucket_size: 5e8
          contiguous_gradients: true
          offload_optimizer:
            device: cpu
          offload_param:
            device: cpu
          overlap_comm: true
          reduce_bucket_size: 5e8
          stage: 3
          stage3_max_reuse_distance: 1e9
          stage3_param_persistence_threshold: 1e6
          stage3_prefetch_bucket_size: 5e8
collator:
  _mode_: call
  _target_: graph_coder.utils.partial
  _var_: collator
  dtype:
    _mode_: call
    _target_: graph_coder.config.F.get_dtype
    _var_: dtype
    name: half
  func:
    _target_: graph_coder.data.collate_ast
  graph_dtype:
    _mode_: call
    _target_: graph_coder.config.F.get_dtype
    _var_: graph_dtype
    name: long
  max_length: 16
  max_seq_length: 2048
  tokenizer:
    _var_: tokenizer
model:
  _target_: graph_coder.models.GraphCoderGenerator
  _var_: model
  decoder:
    _var_: decoder
  embedding:
    _var_: embedding
  encoder:
    _var_: encoder
  eos_token_id: 0
  graph_encoder:
    _var_: graph_encoder
  hidden_size: 64
  max_length: 16
  vocab_size: 50277
runner:
  _target_: graph_coder.runners.GraphCoderGeneratorRunner
  eos_token_id: 0
  model:
    _var_: model
  print_summary: false
  vocab_size: 50277
criterion:
  _target_: torch.nn.CrossEntropyLoss
  _var_: criterion
  ignore_index:
    _var_: tokenizer.pad_token_id
dataset:
  _target_: graph_coder.datasets.AstDataset
  _var_: dataset
  batch_size: 2
  collate_fn:
    _var_: collator
  filter_index:
  - _target_: graph_coder.config.F.filter_is_processed
  - _mode_: call
    _target_: graph_coder.utils.partial
    func:
      _target_: graph_coder.config.F.filter_max_nodes
      max_nodes: 1000
  in_memory: true
  print_summary: true
  random_seed: 42
  root: ~/git-py/raw/python
loggers:
  _var_: loggers
  console:
    _target_: catalyst.dl.ConsoleLogger
  tensorboard:
    _target_: catalyst.dl.TensorboardLogger
    log_batch_metrics: true
    logdir: logs
run:
- _call_: train
  callbacks:
  - _target_: catalyst.callbacks.checkpoint.CheckpointCallback
    loader_key: valid
    logdir: logs
    metric_key: loss
    minimize: true
    topk: 2
  - _target_: catalyst.callbacks.misc.EarlyStoppingCallback
    loader_key: valid
    metric_key: loss
    minimize: true
    patience: 20
  criterion:
    _var_: criterion
  engine:
    _var_: engine
  loaders:
    _var_: dataset.loaders
  logdir: logs
  loggers:
    _var_: loggers
  minimize_valid_metric: true
  num_epochs: 100
  timeit: true
  valid_loader: valid
  valid_metric: loss
  verbose: true
