collator:
  _mode_: call
  _target_: graph_coder.utils.partial
  _var_: collator
  func:
    _target_: graph_coder.data.collate_ast
  max_length: 32
  max_seq_length: 4096
  tokenizer:
    _var_: tokenizer
criterion:
  _target_: torch.nn.CrossEntropyLoss
  _var_: criterion
  ignore_index:
    _var_: tokenizer.pad_token_id
dataset:
  _target_: graph_coder.datasets.AstDataset
  _var_: dataset
  batch_size: 4
  collate_fn:
    _var_: collator
  random_seed: 42
  root: ~/git-py/raw/python
decoder:
  _target_: torch.nn.TransformerDecoder
  _var_: decoder
  decoder_layer:
    _target_: torch.nn.TransformerDecoderLayer
    activation: relu
    d_model: 64
    dim_feedforward: 128
    dropout: 0.1
    nhead: 4
  num_layers: 6
embedding:
  _target_: torch.nn.Embedding
  _var_: embedding
  embedding_dim: 64
  num_embeddings: 50277
  padding_idx: 0
encoder:
  _target_: graph_coder.modules.PerformerEncoder
  _var_: encoder
  causal: true
  depth: 6
  dim: 64
  heads: 4
  max_seq_len: 4096
graph_encoder:
  _target_: graph_coder.modules.TokenGTEncoder
  _var_: graph_encoder
  apply_graphormer_init: true
  attention_dropout: 0
  causal: true
  embedding:
    _var_: embedding
  encoder_attention_heads: 4
  encoder_embed_dim: 64
  encoder_ffn_embed_dim: 128
  encoder_layers: 6
  lap_node_id: true
  performer: true
  type_id: true
model:
  _target_: graph_coder.models.GraphCoderGenerator
  _var_: model
  decoder:
    _var_: decoder
  embedding:
    _var_: embedding
  encoder:
    _var_: encoder
  eos_token_id: 1
  graph_encoder:
    _var_: graph_encoder
  hidden_size: 64
  vocab_size: 50277
optimizer:
  _target_: torch.optim.AdamW
  _var_: optimizer
  lr: 0.02
  params:
    _var_: model.parameters
  weight_decay: 0.01
run:
- _call_: train
  callbacks:
  - _target_: catalyst.callbacks.checkpoint.CheckpointCallback
    loader_key: valid
    logdir: logs
    metric_key: loss
    minimize: true
    topk: 2
  - _target_: catalyst.callbacks.misc.EarlyStoppingCallback
    loader_key: valid
    metric_key: loss
    minimize: true
    patience: 20
  - _target_: catalyst.callbacks.scheduler.SchedulerCallback
    loader_key: valid
    metric_key: valid
  criterion:
    _var_: criterion
  loaders:
    _var_: dataset.loaders
  logdir: logs
  minimize_valid_metric: true
  num_epochs: 100
  optimizer:
    _var_: optimizer
  scheduler:
    _var_: scheduler
  timeit: true
  valid_loader: valid
  valid_metric: loss
  verbose: true
runner:
  _target_: graph_coder.runners.GraphCoderGeneratorRunner
  eos_token_id: 1
  model:
    _var_: model
  vocab_size: 50277
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _var_: scheduler
  optimizer:
    _var_: optimizer
  patience: 10
tokenizer:
  _mode_: call
  _target_: graph_coder.config.F.get_pretrained_tokenizer
  _var_: tokenizer
  eos_token_id: 0
  name: EleutherAI/gpt-neox-20b
  pad_token_id: 1
