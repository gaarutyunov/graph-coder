# GraphCoder: Transformer Pretrained on Code as Graphs

Although software development is mostly a creative process, there are many scrutiny tasks.
As in other industries there is a trend for automation of routine work.
In many cases machine learning and neural networks have become a useful assistant in that matter.
Programming is not an exception â€“ GitHub has stated that Copilot is already used to write up to 30\% code in the company.
Copilot is based on Codex, a Transformer model trained on code as sequence.
However, sequence is not a perfect representation for programming languages.
In this work we claim and demonstrate that by combining the advantages of Transformers
and graph representations of code it is possible to achieve very good results even with comparably small models.

## Citation

```bibtex
@software{graph-coder,
  title = {{GraphCoder: Transformer Pretrained on Code as Graphs}},
  author = {German Arutyunov, Sergey Avdoshin},
  url = {https://www.github.com/gaarutyunov/graph-coder},
}
```