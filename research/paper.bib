
@online{noauthor__nodate,
	title = {Кадровый голод. России не хватает миллиона {IT}-специалистов. На кого пойти учиться, чтобы обеспечить себе будущее?: Интернет: Интернет и СМИ: Lenta.ru},
	url = {https://lenta.ru/articles/2021/07/27/golod/},
	urldate = {2022-03-08},
	file = {Кадровый голод. России не хватает миллиона IT-специалистов. На кого пойти учиться, чтобы обеспечить себе будущее?\: Интернет\: Интернет и СМИ\: Lenta.ru:/Users/germanarutunov/Zotero/storage/KEAVPSYM/golod.html:text/html},
}

@inproceedings{gottschlich_three_2018,
	title = {The three pillars of machine programming},
	pages = {69--80},
	booktitle = {Proceedings of the 2nd {ACM} {SIGPLAN} International Workshop on Machine Learning and Programming Languages},
	author = {Gottschlich, Justin and Solar-Lezama, Armando and Tatbul, Nesime and Carbin, Michael and Rinard, Martin and Barzilay, Regina and Amarasinghe, Saman and Tenenbaum, Joshua B. and Mattson, Tim},
	date = {2018},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/4XNBEIHC/Gottschlich et al. - 2018 - The three pillars of machine programming.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/V9H85RKG/3211346.html:text/html},
}

@online{noauthor_gpt-j-6b_nodate,
	title = {{GPT}-J-6B языковая модель с открытым исходным кодом — Machine learning на vc.ru},
	url = {https://vc.ru/ml/332142-gpt-j-6b-yazykovaya-model-s-otkrytym-ishodnym-kodom},
	urldate = {2022-03-09},
	file = {GPT-J-6B языковая модель с открытым исходным кодом — Machine learning на vc.ru:/Users/germanarutunov/Zotero/storage/N5YQE5PH/332142-gpt-j-6b-yazykovaya-model-s-otkrytym-ishodnym-kodom.html:text/html},
}

@inproceedings{yasunaga_graph-based_2020,
	title = {Graph-based, self-supervised program repair from diagnostic feedback},
	pages = {10799--10808},
	booktitle = {International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Yasunaga, Michihiro and Liang, Percy},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/IZ94CSM8/Yasunaga and Liang - 2020 - Graph-based, self-supervised program repair from d.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/EC9GJMNR/yasunaga20a.html:text/html},
}

@article{nguyen_regvd_2021,
	title = {{ReGVD}: Revisiting Graph Neural Networks for Vulnerability Detection},
	shorttitle = {{ReGVD}},
	journaltitle = {{arXiv} preprint {arXiv}:2110.07317},
	author = {Nguyen, Van-Anh and Nguyen, Dai Quoc and Nguyen, Van and Le, Trung and Tran, Quan Hung and Phung, Dinh},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/D3A7FTZP/Nguyen et al. - 2021 - ReGVD Revisiting Graph Neural Networks for Vulner.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/SBV2EANB/2110.html:text/html},
}

@inproceedings{ganz_explaining_2021,
	title = {Explaining Graph Neural Networks for Vulnerability Discovery},
	pages = {145--156},
	booktitle = {Proceedings of the 14th {ACM} Workshop on Artificial Intelligence and Security},
	author = {Ganz, Tom and Härterich, Martin and Warnecke, Alexander and Rieck, Konrad},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/S93BZPLT/Ganz et al. - 2021 - Explaining Graph Neural Networks for Vulnerability.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/BJLT3PEH/3474369.html:text/html},
}

@inproceedings{hovsepyan_software_2012,
	title = {Software vulnerability prediction using text analysis techniques},
	pages = {7--10},
	booktitle = {Proceedings of the 4th international workshop on Security measurements and metrics},
	author = {Hovsepyan, Aram and Scandariato, Riccardo and Joosen, Wouter and Walden, James},
	date = {2012},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/ID56RUBN/Hovsepyan et al. - 2012 - Software vulnerability prediction using text analy.pdf:application/pdf;Full Text:/Users/germanarutunov/Zotero/storage/AZG4NQ5I/Hovsepyan et al. - 2012 - Software vulnerability prediction using text analy.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/KPS8T2BL/2372225.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/M88Y2YTA/2372225.html:text/html},
}

@inproceedings{pang_predicting_2015,
	title = {Predicting vulnerable software components through n-gram analysis and statistical feature selection},
	pages = {543--548},
	booktitle = {2015 {IEEE} 14th International Conference on Machine Learning and Applications ({ICMLA})},
	publisher = {{IEEE}},
	author = {Pang, Yulei and Xue, Xiaozhen and Namin, Akbar Siami},
	date = {2015},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/I5URAIJS/7424372.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/EDTX4YM9/7424372.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/5G2XDMQ5/7424372.html:text/html},
}

@inproceedings{tian_evaluating_2020,
	title = {Evaluating representation learning of code changes for predicting patch correctness in program repair},
	pages = {981--992},
	booktitle = {2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Tian, Haoye and Liu, Kui and Kaboré, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyandé, Tegawendé F.},
	date = {2020},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/RPW56ZQT/9286101.html:text/html},
}

@article{barone_parallel_2017,
	title = {A parallel corpus of python functions and documentation strings for automated code documentation and code generation},
	journaltitle = {{arXiv} preprint {arXiv}:1707.02275},
	author = {Barone, Antonio Valerio Miceli and Sennrich, Rico},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/9T5N8WSN/Barone and Sennrich - 2017 - A parallel corpus of python functions and document.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/69BBMATT/1707.html:text/html},
}

@inproceedings{nair_funcgnn_2020,
	title = {{FuncGNN}: a graph neural network approach to program similarity},
	shorttitle = {{FuncGNN}},
	pages = {1--11},
	booktitle = {Proceedings of the 14th {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Nair, Aravind and Roy, Avijit and Meinke, Karl},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/759UWNZX/Nair et al. - 2020 - FuncGNN a graph neural network approach to progra.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/GF582F3G/3382494.html:text/html},
}

@inproceedings{wang_detecting_2020,
	title = {Detecting code clones with graph neural network and flow-augmented abstract syntax tree},
	pages = {261--271},
	booktitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
	date = {2020},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/QSDZXZ8Y/9054857.html:text/html},
}

@article{wang_learning_2020,
	title = {Learning to represent programs with heterogeneous graphs},
	journaltitle = {{arXiv} preprint {arXiv}:2012.04188},
	author = {Wang, Wenhan and Zhang, Kechi and Li, Ge and Jin, Zhi},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/VV78KZ8S/Wang et al. - 2020 - Learning to represent programs with heterogeneous .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/79NUZRBB/2012.html:text/html},
}

@online{noauthor_microsoft_nodate,
	title = {Microsoft и {GitHub} представили нейросетевого помощника программиста Copilot},
	url = {https://habr.com/ru/news/t/565376/},
	abstract = {29 июня 2021 года Microsoft и {GitHub} представили нейросетевого помощника программиста Copilot («второй пилот») на базе технологий компании Open {AI}, занимающейся исследованиями в области...},
	titleaddon = {Хабр},
	urldate = {2022-03-09},
	langid = {russian},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/4A9WLTUU/565376.html:text/html},
}

@online{noauthor_amazon_2021,
	title = {Amazon brings automated secrets detection to {CodeGuru}},
	url = {https://venturebeat.com/2021/11/29/amazon-brings-automated-secrets-detection-to-codeguru/},
	abstract = {Amazon has launched a new secrets detection feature that automatically finds private system credentials "hidden" in public source code.},
	titleaddon = {{VentureBeat}},
	urldate = {2022-03-09},
	date = {2021-11-29},
	langid = {american},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/7PWSKU6H/amazon-brings-automated-secrets-detection-to-codeguru.html:text/html},
}

@article{chen_evaluating_2021,
	title = {Evaluating large language models trained on code},
	journaltitle = {{arXiv} preprint {arXiv}:2107.03374},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/I6N6MB9M/Chen et al. - 2021 - Evaluating large language models trained on code.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/JXBFUD6C/2107.html:text/html},
}

@article{sobania_choose_2021,
	title = {Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of {GitHub} Copilot and Genetic Programming},
	shorttitle = {Choose Your Programming Copilot},
	journaltitle = {{arXiv} preprint {arXiv}:2111.07875},
	author = {Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/44VZHYCT/Sobania et al. - 2021 - Choose Your Programming Copilot A Comparison of t.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/TKTFPWPK/2111.html:text/html},
}

@article{cummins_programl_2020,
	title = {Programl: Graph-based deep learning for program optimization and analysis},
	shorttitle = {Programl},
	journaltitle = {{arXiv} preprint {arXiv}:2003.10536},
	author = {Cummins, Chris and Fisches, Zacharias V. and Ben-Nun, Tal and Hoefler, Torsten and Leather, Hugh},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/UV3BGILU/Cummins et al. - 2020 - Programl Graph-based deep learning for program op.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/CPNBHN4J/2003.html:text/html},
}

@online{noauthor_machine_nodate,
	title = {Machine Learning in Static Analysis of Program Source Code},
	url = {https://habr.com/en/company/pvs-studio/blog/484202/},
	abstract = {Machine learning has firmly entrenched in a variety of human fields, from speech recognition to medical diagnosing. The popularity of this approach is so great that people try to use it wherever...},
	titleaddon = {Хабр},
	urldate = {2022-03-09},
	langid = {russian},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/8MHVJFDR/484202.html:text/html},
}

@inproceedings{allamanis_mining_2014,
	title = {Mining idioms from source code},
	pages = {472--483},
	booktitle = {Proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	date = {2014},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/WPIQB9IE/Allamanis and Sutton - 2014 - Mining idioms from source code.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/3ZDVNIY3/2635868.html:text/html},
}

@inproceedings{malik_nl2type_2019,
	title = {{NL}2Type: inferring {JavaScript} function types from natural language information},
	shorttitle = {{NL}2Type},
	pages = {304--315},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
	date = {2019},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/7HW3M3Z6/8811893.html:text/html},
}

@article{schrouff_inferring_2019,
	title = {Inferring javascript types using graph neural networks},
	journaltitle = {{arXiv} preprint {arXiv}:1905.06707},
	author = {Schrouff, Jessica and Wohlfahrt, Kai and Marnette, Bruno and Atkinson, Liam},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/9E8Z2SR9/Schrouff et al. - 2019 - Inferring javascript types using graph neural netw.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/Q32TUY82/1905.html:text/html},
}

@article{wei_lambdanet_2020,
	title = {Lambdanet: Probabilistic type inference using graph neural networks},
	shorttitle = {Lambdanet},
	journaltitle = {{arXiv} preprint {arXiv}:2005.02161},
	author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/843E84EH/Wei et al. - 2020 - Lambdanet Probabilistic type inference using grap.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/4TS67C3G/2005.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \{{\textbackslash}textbackslash\}Lukasz and Polosukhin, Illia},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/KJ6FRDI6/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/ER4QQEBJ/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html:text/html},
}

@article{scarselli_graph_2008,
	title = {The graph neural network model},
	volume = {20},
	pages = {61--80},
	number = {1},
	journaltitle = {{IEEE} transactions on neural networks},
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	date = {2008},
	note = {Publisher: {IEEE}},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/7CDPMPSY/4700287.html:text/html},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: A review of methods and applications},
	volume = {1},
	shorttitle = {Graph neural networks},
	pages = {57--81},
	journaltitle = {{AI} Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	date = {2020},
	note = {Publisher: Elsevier},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/FYDMIUGT/S2666651021000012.html:text/html},
}

@online{noauthor_novel_nodate,
	title = {A Novel Connectionist System for Unconstrained Handwriting Recognition {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/4531750},
	urldate = {2022-03-09},
	file = {A Novel Connectionist System for Unconstrained Handwriting Recognition | IEEE Journals & Magazine | IEEE Xplore:/Users/germanarutunov/Zotero/storage/WPQL4YLR/4531750.html:text/html},
}

@article{graves_novel_2009,
	title = {A Novel Connectionist System for Unconstrained Handwriting Recognition},
	volume = {31},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2008.137},
	abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art {HMM}-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and {HMMs}, suggesting reasons for the network's superior performance.},
	pages = {855--868},
	number = {5},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Graves, Alex and Liwicki, Marcus and Fernández, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, Jürgen},
	date = {2009-05},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {bidirectional long short-term memory, Character recognition, connectionist temporal classification, Connectionist temporal classification, Databases, Handwriting recognition, hidden Markov model., Hidden Markov models, Labeling, Long Short-Term Memory, offline handwriting, Offline handwriting recognition, online handwriting, Online handwriting recognition, recurrent neural networks, Recurrent neural networks, Robustness, Size measurement, Speech, Text recognition, Unconstrained handwriting recognition},
	file = {IEEE Xplore Abstract Record:/Users/germanarutunov/Zotero/storage/SKJAL58G/4531750.html:text/html;Submitted Version:/Users/germanarutunov/Zotero/storage/4BWBECRT/Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf;Submitted Version:/Users/germanarutunov/Zotero/storage/QNCV8QIU/Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf},
}

@inproceedings{grosicki_icdar_2009,
	title = {{ICDAR} 2009 handwriting recognition competition},
	pages = {1398--1402},
	booktitle = {2009 10th International Conference on Document Analysis and Recognition},
	publisher = {{IEEE}},
	author = {Grosicki, Emmanuele and El Abed, Haikal},
	date = {2009},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/5F5WFU9I/5277783.html:text/html},
}

@book{murphy_machine_2012,
	title = {Machine learning: a probabilistic perspective},
	shorttitle = {Machine learning},
	publisher = {{MIT} press},
	author = {Murphy, Kevin P.},
	date = {2012},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/XYFH5RDN/Murphy - 2012 - Machine learning a probabilistic perspective.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/V6RNAP5A/books.html:text/html},
}

@article{bunke_relation_1997,
	title = {On a relation between graph edit distance and maximum common subgraph},
	volume = {18},
	pages = {689--694},
	number = {8},
	journaltitle = {Pattern recognition letters},
	author = {Bunke, Horst},
	date = {1997},
	note = {Publisher: Elsevier},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/HGK4DCXV/Bunke - 1997 - On a relation between graph edit distance and maxi.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/LK6LATW7/S0167865597000603.html:text/html},
}

@online{noauthor_training_nodate,
	title = {A training algorithm for optimal margin classifiers {\textbar} Proceedings of the fifth annual workshop on Computational learning theory},
	url = {https://dl.acm.org/doi/abs/10.1145/130385.130401?casa_token=By_JtcefmdEAAAAA:DYdh3kZoIJLFd8f8PRVb6D3ktavgipnSuvlJ52Aj3kgRXPW3gcEwMgkfhIMERTrlKaBGR9Xsxkh2K4E},
	urldate = {2022-03-09},
	file = {A training algorithm for optimal margin classifiers | Proceedings of the fifth annual workshop on Computational learning theory:/Users/germanarutunov/Zotero/storage/MC44E48C/130385.html:text/html},
}

@inproceedings{boser_training_1992,
	location = {New York, {NY}, {USA}},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {https://doi.org/10.1145/130385.130401},
	doi = {10.1145/130385.130401},
	series = {{COLT} '92},
	abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the {VC}-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
	pages = {144--152},
	booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
	publisher = {Association for Computing Machinery},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	urldate = {2022-03-08},
	date = {1992-07-01},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/4XK64MI4/Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{bahdanau_neural_2016,
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	journaltitle = {{arXiv}:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	urldate = {2022-03-09},
	date = {2016-05-19},
	eprinttype = {arxiv},
	eprint = {1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/SXTSZH9S/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/NPDXVW99/1409.html:text/html},
}

@online{noauthor_goodbye_nodate,
	title = {Goodbye, "free software"; hello, "open source"},
	url = {http://www.catb.org/~esr/open-source.html},
	urldate = {2022-03-09},
	file = {Goodbye, "free software"\; hello, "open source":/Users/germanarutunov/Zotero/storage/LGBZX8SJ/open-source.html:text/html},
}

@article{sanfeliu_distance_1983,
	title = {A distance measure between attributed relational graphs for pattern recognition},
	volume = {{SMC}-13},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1983.6313167},
	abstract = {A method to determine a distance measure between two nonhierarchical attributed relational graphs is presented. In order to apply this distance measure, the graphs are characterised by descriptive graph grammars ({DGG}). The proposed distance measure is based on the computation of the minimum number of modifications required to transform an input graph into the reference one. Specifically, the distance measure is defined as the cost of recognition of nodes plus the number of transformations which include node insertion, node deletion, branch insertion, branch deletion, node label substitution and branch label substitution. The major difference between the proposed distance measure and the other ones is the consideration of the cost of recognition of nodes in the distance computation. In order to do this, the principal features of the nodes are described by one or several cost functions which are used to compute the similarity between the input nodes and the reference ones. Finally, an application of this distance measure to the recognition of lower case handwritten English characters is presented.},
	pages = {353--362},
	number = {3},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics},
	author = {Sanfeliu, Alberto and Fu, King-Sun},
	date = {1983-05},
	note = {Conference Name: {IEEE} Transactions on Systems, Man, and Cybernetics},
	keywords = {Cost function, Grammar, Measurement uncertainty, Merging, Pattern recognition, Transforms},
	file = {IEEE Xplore Abstract Record:/Users/germanarutunov/Zotero/storage/SP4ZAGQV/6313167.html:text/html},
}

@article{lipton_critical_2015,
	title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	url = {http://arxiv.org/abs/1506.00019},
	abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks ({RNNs}) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory ({LSTM}) and bidirectional ({BRNN}) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
	journaltitle = {{arXiv}:1506.00019 [cs]},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	urldate = {2022-03-09},
	date = {2015-10-17},
	eprinttype = {arxiv},
	eprint = {1506.00019},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/P3SRMIUK/Lipton et al. - 2015 - A Critical Review of Recurrent Neural Networks for.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/9LPIXEAK/1506.html:text/html},
}

@online{foundation_eclipse_nodate,
	title = {Eclipse {IDE} 2021-12 {\textbar} The Eclipse Foundation},
	url = {https://www.eclipse.org/eclipseide/},
	abstract = {The Eclipse Foundation - home to a global community, the Eclipse {IDE}, Jakarta {EE} and over 415 open source projects, including runtimes, tools and frameworks.},
	author = {Foundation, Eclipse},
	urldate = {2022-03-09},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/IAKHLWTI/eclipseide.html:text/html},
}

@online{noauthor_build_nodate,
	title = {Build software better, together},
	url = {https://github.com},
	abstract = {{GitHub} is where people build software. More than 73 million people use {GitHub} to discover, fork, and contribute to over 200 million projects.},
	titleaddon = {{GitHub}},
	urldate = {2022-03-09},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/9WCZLXRI/github.com.html:text/html},
}

@article{lecun_generalization_1989,
	title = {Generalization and network design strategies},
	volume = {19},
	pages = {18},
	number = {143},
	journaltitle = {Connectionism in perspective},
	author = {{LeCun}, Yann},
	date = {1989},
	note = {Publisher: North Holland},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/EH62X5UF/LeCun - 1989 - Generalization and network design strategies.pdf:application/pdf},
}

@incollection{lecun_object_1999,
	title = {Object recognition with gradient-based learning},
	pages = {319--345},
	booktitle = {Shape, contour and grouping in computer vision},
	publisher = {Springer},
	author = {{LeCun}, Yann and Haffner, Patrick and Bottou, Léon and Bengio, Yoshua},
	date = {1999},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/7EBVRBRY/LeCun et al. - 1999 - Object recognition with gradient-based learning.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/IFT4EQ9U/3-540-46805-6_19.html:text/html},
}

@online{noauthor_github_nodate,
	title = {{GitHub} Copilot · Your {AI} pair programmer},
	url = {https://copilot.github.com/},
	abstract = {{GitHub} Copilot works alongside you directly in your editor, suggesting whole lines or entire functions for you.},
	titleaddon = {{GitHub} Copilot},
	urldate = {2022-03-09},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/VNBV5LHK/copilot.github.com.html:text/html},
}

@software{wang_table_2022,
	title = {Table of contents},
	rights = {Apache-2.0},
	url = {https://github.com/kingoflolz/mesh-transformer-jax},
	abstract = {Model parallel transformers in {JAX} and Haiku},
	author = {Wang, Ben},
	urldate = {2022-03-09},
	date = {2022-03-09},
	note = {original-date: 2021-03-13T23:31:13Z},
}

@online{noauthor_openai_nodate,
	title = {{OpenAI}},
	url = {https://openai.com/},
	abstract = {{OpenAI} is an {AI} research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.},
	titleaddon = {{OpenAI}},
	urldate = {2022-03-09},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/6QQCEWSY/openai.com.html:text/html},
}

@online{noauthor_stack_nodate,
	title = {Stack Overflow на русском},
	url = {https://ru.stackoverflow.com/},
	abstract = {Вопросы и ответы для программистов},
	titleaddon = {Stack Overflow на русском},
	urldate = {2022-03-09},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/4SCV37V3/ru.stackoverflow.com.html:text/html},
}

@article{hindle_naturalness_2016,
	title = {On the naturalness of software},
	volume = {59},
	pages = {122--131},
	number = {5},
	journaltitle = {Communications of the {ACM}},
	author = {Hindle, Abram and Barr, Earl T. and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
	date = {2016},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/C4ZM5PRA/Hindle et al. - 2016 - On the naturalness of software.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/CGBPEI6Q/2902362.html:text/html},
}

@inproceedings{nguyen_statistical_2013,
	title = {A statistical semantic language model for source code},
	pages = {532--542},
	booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
	author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
	date = {2013},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/DEU2U3CV/Nguyen et al. - 2013 - A statistical semantic language model for source c.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/N5WI7WSN/2491411.html:text/html},
}

@inproceedings{raychev_code_2014,
	title = {Code completion with statistical language models},
	pages = {419--428},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
	date = {2014},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/BREPHVXF/Raychev et al. - 2014 - Code completion with statistical language models.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/IIA5XJIE/2594291.html:text/html},
}

@article{li_code_2017,
	title = {Code completion with neural attention and pointer networks},
	journaltitle = {{arXiv} preprint {arXiv}:1711.09573},
	author = {Li, Jian and Wang, Yue and Lyu, Michael R. and King, Irwin},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/8K8VMJWM/Li et al. - 2017 - Code completion with neural attention and pointer .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/L3LALAMP/1711.html:text/html},
}

@article{wang_code_2021,
	title = {Code completion by modeling flattened abstract syntax trees as graphs},
	journaltitle = {Proceedings of {AAAIConference} on Artificial Intellegence},
	author = {Wang, Yanlin and Li, Hui},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/84VX4RDB/Wang and Li - 2021 - Code completion by modeling flattened abstract syn.pdf:application/pdf},
}

@inproceedings{svyatkovskiy_intellicode_2020,
	title = {Intellicode compose: Code generation using transformer},
	shorttitle = {Intellicode compose},
	pages = {1433--1443},
	booktitle = {Proceedings of the 28th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/YJWFQQAE/Svyatkovskiy et al. - 2020 - Intellicode compose Code generation using transfo.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/F949FZWH/3368089.html:text/html},
}

@article{zhou_improving_2021,
	title = {Improving Code Autocompletion with Transfer Learning},
	journaltitle = {{arXiv} preprint {arXiv}:2105.05991},
	author = {Zhou, Wen and Kim, Seohyun and Murali, Vijayaraghavan and Aye, Gareth Ari},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/XGJY93SJ/Zhou et al. - 2021 - Improving Code Autocompletion with Transfer Learni.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/N5I69XDE/2105.html:text/html},
}

@inproceedings{allamanis_learning_2014,
	title = {Learning natural coding conventions},
	pages = {281--293},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles},
	date = {2014},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/2ESF6F2E/Allamanis et al. - 2014 - Learning natural coding conventions.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/5R5TZ9BK/2635868.html:text/html},
}

@article{amodio_neural_2017,
	title = {Neural attribute machines for program generation},
	journaltitle = {{arXiv} preprint {arXiv}:1705.09231},
	author = {Amodio, Matthew and Chaudhuri, Swarat and Reps, Thomas W.},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/TIBDB6UE/Amodio et al. - 2017 - Neural attribute machines for program generation.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/K2SPS2VZ/1705.html:text/html},
}

@article{ling_latent_2016,
	title = {Latent predictor networks for code generation},
	journaltitle = {{arXiv} preprint {arXiv}:1603.06744},
	author = {Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Kočiskỳ, Tomáš and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/UKTW9PG5/Ling et al. - 2016 - Latent predictor networks for code generation.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/4BQZPG8R/1603.html:text/html},
}

@inproceedings{movshovitz-attias_natural_2013,
	title = {Natural language models for predicting programming comments},
	pages = {35--40},
	booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Movshovitz-Attias, Dana and Cohen, William},
	date = {2013},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/838G4AGF/Movshovitz-Attias and Cohen - 2013 - Natural language models for predicting programming.pdf:application/pdf},
}

@inproceedings{liu_towards_2016,
	title = {Towards better program obfuscation: Optimization via language models},
	shorttitle = {Towards better program obfuscation},
	pages = {680--682},
	booktitle = {2016 {IEEE}/{ACM} 38th International Conference on Software Engineering Companion ({ICSE}-C)},
	publisher = {{IEEE}},
	author = {Liu, Han},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/8ZR5H7HF/Liu - 2016 - Towards better program obfuscation Optimization v.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/BMNEIPQK/7883371.html:text/html},
}

@article{kamiya_ccfinder_2002,
	title = {{CCFinder}: A multilinguistic token-based code clone detection system for large scale source code},
	volume = {28},
	shorttitle = {{CCFinder}},
	pages = {654--670},
	number = {7},
	journaltitle = {{IEEE} transactions on software engineering},
	author = {Kamiya, Toshihiro and Kusumoto, Shinji and Inoue, Katsuro},
	date = {2002},
	note = {Publisher: {IEEE}},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/LJNIQSBC/1019480.html:text/html},
}

@inproceedings{hellendoorn_deep_2018,
	title = {Deep learning type inference},
	pages = {152--162},
	booktitle = {Proceedings of the 2018 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
	date = {2018},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/6SS29NBY/Hellendoorn et al. - 2018 - Deep learning type inference.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/6SRU8D2Q/3236024.html:text/html},
}

@article{mir_type4py_2021,
	title = {Type4py: Deep similarity learning-based type inference for python},
	shorttitle = {Type4py},
	journaltitle = {{arXiv} preprint {arXiv}:2101.04470},
	author = {Mir, Amir M. and Latoskinas, Evaldas and Proksch, Sebastian and Gousios, Georgios},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/Z33GMLSQ/Mir et al. - 2021 - Type4py Deep similarity learning-based type infer.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/P843D6IM/2101.html:text/html},
}

@article{karampatsis_scelmo_2020,
	title = {Scelmo: Source code embeddings from language models},
	shorttitle = {Scelmo},
	journaltitle = {{arXiv} preprint {arXiv}:2004.13214},
	author = {Karampatsis, Rafael-Michael and Sutton, Charles},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/2I8DJQ2V/Karampatsis and Sutton - 2020 - Scelmo Source code embeddings from language model.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/ZI5HTQNY/2004.html:text/html},
}

@article{pradel_deepbugs_2018,
	title = {Deepbugs: A learning approach to name-based bug detection},
	volume = {2},
	shorttitle = {Deepbugs},
	pages = {1--25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	author = {Pradel, Michael and Sen, Koushik},
	date = {2018},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/L52S4LV2/Pradel and Sen - 2018 - Deepbugs A learning approach to name-based bug de.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/58PFJ5IA/3276517.html:text/html},
}

@inproceedings{wang_bugram_2016,
	title = {Bugram: bug detection with n-gram language models},
	shorttitle = {Bugram},
	pages = {708--719},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering},
	author = {Wang, Song and Chollak, Devin and Movshovitz-Attias, Dana and Tan, Lin},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/HRAL5K6A/Wang et al. - 2016 - Bugram bug detection with n-gram language models.pdf:application/pdf;Full Text:/Users/germanarutunov/Zotero/storage/T23IRLUV/Wang et al. - 2016 - Bugram bug detection with n-gram language models.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/MMKQ5PYE/2970276.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/9YWKVZLF/2970276.html:text/html},
}

@article{cabrera_lozoya_commit2vec_2021,
	title = {Commit2vec: Learning distributed representations of code changes},
	volume = {2},
	shorttitle = {Commit2vec},
	pages = {1--16},
	number = {3},
	journaltitle = {{SN} Computer Science},
	author = {Cabrera Lozoya, Rocío and Baumann, Arnaud and Sabetta, Antonino and Bezzi, Michele},
	date = {2021},
	note = {Publisher: Springer},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/UBC8YLZH/Cabrera Lozoya et al. - 2021 - Commit2vec Learning distributed representations o.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/WTBCFYXB/s42979-021-00566-z.html:text/html},
}

@inproceedings{mou_convolutional_2016,
	title = {Convolutional neural networks over tree structures for programming language processing},
	booktitle = {Thirtieth {AAAI} conference on artificial intelligence},
	author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/WYHECX7L/Mou et al. - 2016 - Convolutional neural networks over tree structures.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/7UK9K3RR/11775.html:text/html},
}

@inproceedings{white_deep_2016,
	title = {Deep learning code fragments for code clone detection},
	pages = {87--98},
	booktitle = {2016 31st {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys},
	date = {2016},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/IJVFDQYC/7582748.html:text/html},
}

@inproceedings{wei_supervised_2017,
	title = {Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical Information in Source Code.},
	pages = {3034--3040},
	booktitle = {{IJCAI}},
	author = {Wei, Huihui and Li, Ming},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/FW7J3Q2R/Wei and Li - 2017 - Supervised Deep Features for Software Functional C.pdf:application/pdf;Full Text:/Users/germanarutunov/Zotero/storage/AIZT9966/Wei and Li - 2017 - Supervised Deep Features for Software Functional C.pdf:application/pdf},
}

@inproceedings{russell_automated_2018,
	title = {Automated vulnerability detection in source code using deep representation learning},
	pages = {757--762},
	booktitle = {2018 17th {IEEE} international conference on machine learning and applications ({ICMLA})},
	publisher = {{IEEE}},
	author = {Russell, Rebecca and Kim, Louis and Hamilton, Lei and Lazovich, Tomo and Harer, Jacob and Ozdemir, Onur and Ellingwood, Paul and {McConley}, Marc},
	date = {2018},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/E9SB3EL6/8614145.html:text/html},
}

@online{noauthor_180101681_nodate,
	title = {[1801.01681] {VulDeePecker}: A Deep Learning-Based System for Vulnerability Detection},
	url = {https://arxiv.org/abs/1801.01681},
	urldate = {2022-03-09},
	file = {[1801.01681] VulDeePecker\: A Deep Learning-Based System for Vulnerability Detection:/Users/germanarutunov/Zotero/storage/4Z5NDCF3/1801.html:text/html},
}

@online{noauthor_sysevr_nodate,
	title = {{SySeVR}: A Framework for Using Deep Learning to Detect Software Vulnerabilities {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/abstract/document/9321538?casa_token=4ufWfbN8GAsAAAAA:JjS9Lyf7jRZLb85EqaRrj1lO9FX2q0EsLnwlZBqsFjisRTJtm0Ma6n7-YG4sVX2O4YfjAw8HnQ4},
	urldate = {2022-03-09},
	file = {SySeVR\: A Framework for Using Deep Learning to Detect Software Vulnerabilities | IEEE Journals & Magazine | IEEE Xplore:/Users/germanarutunov/Zotero/storage/XT6T9CL2/9321538.html:text/html},
}

@article{li_sysevr_2021,
	title = {{SySeVR}: A Framework for Using Deep Learning to Detect Software Vulnerabilities},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2021.3051525},
	shorttitle = {{SySeVR}},
	abstract = {The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations ({SySeVR}), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been “silently” patched by the vendors when releasing newer versions of the pertinent software products.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Dependable and Secure Computing},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Jin, Hai and Zhu, Yawei and Chen, Zhaoxuan},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Dependable and Secure Computing},
	keywords = {Big Data, deep learning, Deep learning, Image processing, program analysis, program representation, Proposals, security, Semantics, Software, Syntactics, Vulnerability detection},
	file = {IEEE Xplore Full Text PDF:/Users/germanarutunov/Zotero/storage/JS8A9YM7/Li et al. - 2021 - SySeVR A Framework for Using Deep Learning to Det.pdf:application/pdf},
}

@online{noauthor_code2vec_nodate,
	title = {code2vec: learning distributed representations of code {\textbar} Proceedings of the {ACM} on Programming Languages},
	url = {https://dl.acm.org/doi/abs/10.1145/3290353},
	urldate = {2022-03-09},
	file = {code2vec\: learning distributed representations of code | Proceedings of the ACM on Programming Languages:/Users/germanarutunov/Zotero/storage/EMYRTFB3/3290353.html:text/html},
}

@article{alon_code2vec_2019,
	title = {code2vec: learning distributed representations of code},
	volume = {3},
	url = {https://doi.org/10.1145/3290353},
	doi = {10.1145/3290353},
	shorttitle = {code2vec},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75\%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
	pages = {40:1--40:29},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2022-03-09},
	date = {2019-01-02},
	keywords = {Big Code, Distributed Representations, Machine Learning},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/XIQ3GQ7F/Alon et al. - 2019 - code2vec learning distributed representations of .pdf:application/pdf},
}

@article{bavishi_autopandas_2019,
	title = {{AutoPandas}: neural-backed generators for program synthesis},
	volume = {3},
	shorttitle = {{AutoPandas}},
	pages = {1--27},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	author = {Bavishi, Rohan and Lemieux, Caroline and Fox, Roy and Sen, Koushik and Stoica, Ion},
	date = {2019},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/6AMLFHRN/Bavishi et al. - 2019 - AutoPandas neural-backed generators for program s.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/3GM39RJA/3360594.html:text/html},
}

@article{liu_haconvgnn_2021,
	title = {{HAConvGNN}: Hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks},
	shorttitle = {{HAConvGNN}},
	journaltitle = {{arXiv} preprint {arXiv}:2104.01002},
	author = {Liu, Xuye and Wang, Dakuo and Wang, April and Hou, Yufang and Wu, Lingfei},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/F9U66JRM/Liu et al. - 2021 - HAConvGNN Hierarchical attention based convolutio.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/ISLG35X3/2104.html:text/html},
}

@article{li_vuldeepecker_2018,
	title = {Vuldeepecker: A deep learning-based system for vulnerability detection},
	shorttitle = {Vuldeepecker},
	journaltitle = {{arXiv} preprint {arXiv}:1801.01681},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Ou, Xinyu and Jin, Hai and Wang, Sujuan and Deng, Zhijun and Zhong, Yuyi},
	date = {2018},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/PINXQ8TK/Li et al. - 2018 - Vuldeepecker A deep learning-based system for vul.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/2GE9GCZ3/1801.html:text/html},
}

@inproceedings{bai_simgnn_2019,
	title = {Simgnn: A neural network approach to fast graph similarity computation},
	shorttitle = {Simgnn},
	pages = {384--392},
	booktitle = {Proceedings of the Twelfth {ACM} International Conference on Web Search and Data Mining},
	author = {Bai, Yunsheng and Ding, Hao and Bian, Song and Chen, Ting and Sun, Yizhou and Wang, Wei},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/9KUCCDDA/Bai et al. - 2019 - Simgnn A neural network approach to fast graph si.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/YLJCBHHY/3289600.html:text/html},
}

@article{zhou_devign_2019,
	title = {Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
	volume = {32},
	shorttitle = {Devign},
	journaltitle = {Advances in neural information processing systems},
	author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/L5ZJXTRY/Zhou et al. - 2019 - Devign Effective vulnerability identification by .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/ZRLS7Q37/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html:text/html},
}

@article{cao_bgnn4vd_2021,
	title = {Bgnn4vd: constructing bidirectional graph neural-network for vulnerability detection},
	volume = {136},
	shorttitle = {Bgnn4vd},
	pages = {106576},
	journaltitle = {Information and Software Technology},
	author = {Cao, Sicong and Sun, Xiaobing and Bo, Lili and Wei, Ying and Li, Bin},
	date = {2021},
	note = {Publisher: Elsevier},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/KZ6QNAVA/S0950584921000586.html:text/html},
}

@article{allamanis_learning_2017,
	title = {Learning to represent programs with graphs},
	journaltitle = {{arXiv} preprint {arXiv}:1711.00740},
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/JZRJFDQQ/Allamanis et al. - 2017 - Learning to represent programs with graphs.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/RNSSDULQ/1711.html:text/html},
}

@inproceedings{sun_treegen_2020,
	title = {Treegen: A tree-based transformer architecture for code generation},
	volume = {34},
	shorttitle = {Treegen},
	pages = {8984--8991},
	booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Sun, Zeyu and Zhu, Qihao and Xiong, Yingfei and Sun, Yican and Mou, Lili and Zhang, Lu},
	date = {2020},
	note = {Issue: 05},
}

@article{feng_codebert_2020,
	title = {Codebert: A pre-trained model for programming and natural languages},
	shorttitle = {Codebert},
	journaltitle = {{arXiv} preprint {arXiv}:2002.08155},
	author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/NG9Z5LCT/Feng et al. - 2020 - Codebert A pre-trained model for programming and .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/WF4885GX/2002.html:text/html},
}

@article{zeng_fast_2019,
	title = {Fast code clone detection based on weighted recursive autoencoders},
	volume = {7},
	pages = {125062--125078},
	journaltitle = {{IEEE} Access},
	author = {Zeng, Jie and Ben, Kerong and Li, Xiaowei and Zhang, Xian},
	date = {2019},
	note = {Publisher: {IEEE}},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/FSSHLRXX/8822436.html:text/html},
}

@inproceedings{kanade_learning_2020,
	title = {Learning and evaluating contextual embedding of source code},
	pages = {5110--5121},
	booktitle = {International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/EN2QQVCZ/Kanade et al. - 2020 - Learning and evaluating contextual embedding of so.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/MENCBIX9/kanade20a.html:text/html},
}

@inproceedings{chen_program_2020,
	title = {Program synthesis using deduction-guided reinforcement learning},
	pages = {587--610},
	booktitle = {International Conference on Computer Aided Verification},
	publisher = {Springer},
	author = {Chen, Yanju and Wang, Chenglong and Bastani, Osbert and Dillig, Isil and Feng, Yu},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/5QBBRFDV/978-3-030-53291-8_30.html:text/html},
}

@article{feng_program_2018,
	title = {Program synthesis using conflict-driven learning},
	volume = {53},
	pages = {420--435},
	number = {4},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	author = {Feng, Yu and Martins, Ruben and Bastani, Osbert and Dillig, Isil},
	date = {2018},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/5KUTFTMD/Feng et al. - 2018 - Program synthesis using conflict-driven learning.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/LMAZEU7C/3296979.html:text/html},
}

@online{noauthor_neo_nodate,
	title = {Neo},
	url = {https://utopia-group.github.io/neo/},
	urldate = {2022-03-12},
	file = {Neo:/Users/germanarutunov/Zotero/storage/AMFCDMBC/neo.html:text/html},
}

@article{feng_component-based_2017,
	title = {Component-based synthesis of table consolidation and transformation tasks from examples},
	volume = {52},
	pages = {422--436},
	number = {6},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	author = {Feng, Yu and Martins, Ruben and Van Geffen, Jacob and Dillig, Isil and Chaudhuri, Swarat},
	date = {2017},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/ZUL2RH8C/Feng et al. - 2017 - Component-based synthesis of table consolidation a.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/E6WD7MIT/3140587.html:text/html},
}

@online{noauthor_morpheus_nodate,
	title = {Morpheus},
	url = {https://utopia-group.github.io/morpheus/},
	urldate = {2022-03-12},
	file = {Morpheus:/Users/germanarutunov/Zotero/storage/AIMUTQV4/morpheus.html:text/html},
}

@article{balog_deepcoder_2016,
	title = {Deepcoder: Learning to write programs},
	shorttitle = {Deepcoder},
	journaltitle = {{arXiv} preprint {arXiv}:1611.01989},
	author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/4KUWRZGS/Balog et al. - 2016 - Deepcoder Learning to write programs.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/7LJBZTMB/1611.html:text/html},
}

@article{bhatia_automated_2016,
	title = {Automated correction for syntax errors in programming assignments using recurrent neural networks},
	journaltitle = {{arXiv} preprint {arXiv}:1603.06129},
	author = {Bhatia, Sahil and Singh, Rishabh},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/5IIYF3PD/Bhatia and Singh - 2016 - Automated correction for syntax errors in programm.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/NN6CJBGK/1603.html:text/html},
}

@online{noauthor_hearthstone_nodate,
	title = {Hearthstone},
	url = {https://playhearthstone.com/ru-ru},
	urldate = {2022-03-13},
	file = {Hearthstone:/Users/germanarutunov/Zotero/storage/6HDG53GE/ru-ru.html:text/html},
}

@article{allamanis_survey_2018,
	title = {A survey of machine learning for big code and naturalness},
	volume = {51},
	pages = {1--37},
	number = {4},
	journaltitle = {{ACM} Computing Surveys ({CSUR})},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	date = {2018},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/E72FDTI4/Allamanis et al. - 2018 - A survey of machine learning for big code and natu.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/R3H8SJPL/3212695.html:text/html},
}

@article{christopher_evaluating_2006,
	title = {Evaluating static analysis frameworks},
	pages = {1--17},
	journaltitle = {Analysis, pág},
	author = {Christopher, Ciera Nicole},
	date = {2006},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/EIIXC97I/Christopher - 2006 - Evaluating static analysis frameworks.pdf:application/pdf},
}

@inproceedings{jiang_treebert_2021,
	title = {{TreeBERT}: A tree-based pre-trained model for programming language},
	shorttitle = {{TreeBERT}},
	pages = {54--63},
	booktitle = {Uncertainty in Artificial Intelligence},
	publisher = {{PMLR}},
	author = {Jiang, Xue and Zheng, Zhuoran and Lyu, Chen and Li, Liang and Lyu, Lei},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/KJA6P4DW/Jiang et al. - 2021 - TreeBERT A tree-based pre-trained model for progr.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/L2ZQVJKW/jiang21a.html:text/html},
}

@online{uri_alon_code2vec_2022,
	title = {Code2vec},
	rights = {{MIT}},
	url = {https://github.com/tech-srl/code2vec},
	abstract = {{TensorFlow} code for the neural network presented in the paper: "code2vec: Learning Distributed Representations of Code"},
	author = {Uri Alon and {Meital Zilberstein} and {Omer Levy} and {Eran Yahav}},
	urldate = {2022-03-13},
	date = {2022-03-04},
	note = {original-date: 2018-07-24T03:40:20Z},
	keywords = {code, code2vec, distributed, learning, of, representations, technion},
}

@inproceedings{abdelaziz_toolkit_2021,
	title = {A toolkit for generating code knowledge graphs},
	pages = {137--144},
	booktitle = {Proceedings of the 11th on Knowledge Capture Conference},
	author = {Abdelaziz, Ibrahim and Dolby, Julian and {McCusker}, Jamie and Srinivas, Kavitha},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/IH82DXS8/Abdelaziz et al. - 2021 - A toolkit for generating code knowledge graphs.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/X3US5TIF/3460210.html:text/html},
}

@online{bavishi_rbavishiautopandas_2022,
	title = {rbavishi/autopandas},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/rbavishi/autopandas},
	abstract = {{OOPSLA} 2019 Artifact for {AutoPandas}. Website at https://rbavishi.github.io/autopandas},
	author = {Bavishi, Rohan},
	urldate = {2022-03-13},
	date = {2022-02-08},
	note = {original-date: 2020-06-25T00:44:34Z},
	keywords = {graph-neural-networks, neural-networks, program-synthesis},
}

@online{noauthor_pandas_nodate,
	title = {pandas - Python Data Analysis Library},
	url = {https://pandas.pydata.org/},
	urldate = {2022-03-13},
	file = {pandas - Python Data Analysis Library:/Users/germanarutunov/Zotero/storage/D7IWZJKD/pandas.pydata.org.html:text/html},
}

@article{sanfeliu_distance_1983-1,
	title = {A distance measure between attributed relational graphs for pattern recognition},
	pages = {353--362},
	number = {3},
	journaltitle = {{IEEE} transactions on systems, man, and cybernetics},
	author = {Sanfeliu, Alberto and Fu, King-Sun},
	date = {1983},
	note = {Publisher: {IEEE}},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/3GKG56TU/6313167.html:text/html},
}

@software{nair_aravi11funcgnn_2022,
	title = {aravi11/{funcGNN}},
	rights = {{GPL}-3.0},
	url = {https://github.com/aravi11/funcGNN},
	abstract = {{CFG} based program similarity using Graph Neural Networks},
	author = {Nair, Aravind},
	urldate = {2022-03-13},
	date = {2022-02-26},
	note = {original-date: 2020-04-06T17:07:26Z},
}

@article{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/RZ4SDWTT/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/X6PALELU/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html:text/html},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-supervised classification with graph convolutional networks},
	journaltitle = {{arXiv} preprint {arXiv}:1609.02907},
	author = {Kipf, Thomas N. and Welling, Max},
	date = {2016},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/MY6IHFQ7/Kipf and Welling - 2016 - Semi-supervised classification with graph convolut.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/BGLVUPL4/1609.html:text/html},
}

@online{__2021,
	title = {Уровень опасности — 10 баллов из 10: уязвимость Log4Shell угрожает миллионам серверов по всему миру — Технологии на {TJ}},
	url = {https://tjournal.ru/tech/492481-uroven-opasnosti-10-ballov-iz-10-uyazvimost-log4shell-ugrozhaet-millionam-serverov-po-vsemu-miru},
	shorttitle = {Уровень опасности — 10 баллов из 10},
	abstract = {Злоумышленники могут получить удаленный контроль над серверами и веб-приложениями, а также компьютерами и гаджетами пользователей: в большинстве случаев специальные знания для такой атаки не нужны.},
	titleaddon = {{TJ}},
	author = {Шевелев, Илья},
	urldate = {2022-03-13},
	date = {2021-12-14},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/B7KXCMSY/492481-uroven-opasnosti-10-ballov-iz-10-uyazvimost-log4shell-ugrozhaet-millionam-serverov-po-vs.html:text/html},
}

@software{nguyen_revisiting_2022,
	title = {Revisiting Graph Neural Networks for Vulnerability Detection},
	url = {https://github.com/daiquocnguyen/GNN-ReGVD},
	abstract = {Revisiting Graph Neural Networks for Vulnerability Detection ({ICSE} 2022) (Pytorch)},
	author = {Nguyen, Dai Quoc},
	urldate = {2022-03-13},
	date = {2022-03-12},
	note = {original-date: 2021-09-13T02:42:09Z},
	keywords = {graph-neural-networks, cyber-security, graph-representation-learning, software-vulnerability, vulnerability-detection, vulnerability-identification},
}

@online{noauthor_google_nodate,
	title = {Google Scholar},
	url = {https://scholar.google.com/scholar?oi=gsb05&lookup_url=http%3A%2F%2Feaidata.bmk.sh%2Fdata%2FGPT_NeoX_20B.pdf&lookup=0&hl=en},
	urldate = {2022-03-17},
	file = {Google Scholar:/Users/germanarutunov/Zotero/storage/WN8PKDAN/scholar.html:text/html},
}

@article{li_competition-level_nodate,
	title = {Competition-Level Code Generation with {AlphaCode}},
	pages = {74},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and de, Cyprien},
	langid = {english},
	file = {Li et al. - Competition-Level Code Generation with AlphaCode.pdf:/Users/germanarutunov/Zotero/storage/WX6CA5LK/Li et al. - Competition-Level Code Generation with AlphaCode.pdf:application/pdf},
}

@article{hendrycks_measuring_2021,
	title = {Measuring coding challenge competence with apps},
	journaltitle = {{arXiv} preprint {arXiv}:2105.09938},
	author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/J2IA5WVY/Hendrycks et al. - 2021 - Measuring coding challenge competence with apps.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/IEU33N92/2105.html:text/html},
}

@online{black_gpt-neo_2021,
	title = {{GPT}-Neo: Large scale autoregressive language modeling with meshtensorflow},
	url = {https://zenodo.org/record/5551208},
	shorttitle = {{GPT}-Neo},
	abstract = {An implementation of model parallel {GPT}-2 and {GPT}-3-style models using the mesh-tensorflow library.},
	author = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
	urldate = {2022-03-17},
	date = {2021-10-06},
	doi = {10.5281/zenodo.5551208},
	file = {Zenodo Snapshot:/Users/germanarutunov/Zotero/storage/W4MUXLBX/5551208.html:text/html},
}

@software{wang_table_2022-1,
	title = {Table of contents},
	rights = {Apache-2.0},
	url = {https://github.com/kingoflolz/mesh-transformer-jax},
	abstract = {Model parallel transformers in {JAX} and Haiku},
	author = {Wang, Ben},
	urldate = {2022-03-17},
	date = {2022-03-16},
	note = {original-date: 2021-03-13T23:31:13Z},
}

@inproceedings{noauthor_notitle_nodate,
}

@misc{wang_gpt-j-6b_2021,
	title = {{GPT}-J-6B: A 6 Billion Parameter Autoregressive Language Model},
	url = {https://github.com/kingoflolz/mesh-transformer-jax},
	author = {Wang, Ben and Komatsuzaki, Aran},
	date = {2021-05},
}

@article{austin_program_2021,
	title = {Program synthesis with large language models},
	journaltitle = {{arXiv} preprint {arXiv}:2108.07732},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/9KPZN4EJ/Austin et al. - 2021 - Program synthesis with large language models.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/IFUZZJRU/2108.html:text/html},
}

@article{clement_pymt5_2020,
	title = {{PyMT}5: multi-mode translation of natural language and Python code with transformers},
	shorttitle = {{PyMT}5},
	journaltitle = {{arXiv} preprint {arXiv}:2010.03150},
	author = {Clement, Colin B. and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/PBYRS63S/Clement et al. - 2020 - PyMT5 multi-mode translation of natural language .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/8JW5QFPU/2010.html:text/html},
}

@article{li_gated_2015,
	title = {Gated graph sequence neural networks},
	journaltitle = {{arXiv} preprint {arXiv}:1511.05493},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	date = {2015},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/25DMBETQ/Li et al. - 2015 - Gated graph sequence neural networks.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/YMQF8QGF/1511.html:text/html},
}

@article{velickovic_graph_2017,
	title = {Graph attention networks},
	journaltitle = {{arXiv} preprint {arXiv}:1710.10903},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
	date = {2017},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/A5RPEQ8S/Veličković et al. - 2017 - Graph attention networks.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/LLX9J8CH/1710.html:text/html},
}

@online{noauthor_pypl_nodate,
	title = {{PYPL} {PopularitY} of Programming Language index},
	url = {https://pypl.github.io/PYPL.html},
	abstract = {{PYPL} popularity of programming language},
	urldate = {2022-03-17},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/IKPY7UFG/PYPL.html:text/html},
}

@software{wei_mrvplusonelambdanet_2022,
	title = {{MrVPlusOne}/{LambdaNet}},
	url = {https://github.com/MrVPlusOne/LambdaNet},
	abstract = {Probabilistic Type Inference using Graph Neural Networks},
	author = {Wei, Jiayi},
	urldate = {2022-03-17},
	date = {2022-02-28},
	note = {original-date: 2018-11-01T04:08:13Z},
	keywords = {graph-neural-networks, iclr2020, pointer-networks, type-inference},
}

@online{sun_treegen_2022,
	title = {{TreeGen}},
	rights = {{MIT}},
	url = {https://github.com/zysszy/TreeGen},
	abstract = {A Tree-Based Transformer Architecture for Code Generation. ({AAAI}'20)},
	author = {Sun, Zeyu},
	urldate = {2022-03-17},
	date = {2022-03-16},
	note = {original-date: 2019-11-20T05:52:50Z},
}

@inproceedings{sun_grammar-based_2019,
	title = {A grammar-based structural cnn decoder for code generation},
	volume = {33},
	pages = {7055--7062},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Sun, Zeyu and Zhu, Qihao and Mou, Lili and Xiong, Yingfei and Li, Ge and Zhang, Lu},
	date = {2019},
	note = {Issue: 01},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/HCGGHENX/Sun et al. - 2019 - A grammar-based structural cnn decoder for code ge.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/DVA49KU9/4686.html:text/html},
}

@online{noauthor_grammar_nodate,
	title = {A Grammar of Data Manipulation},
	url = {https://dplyr.tidyverse.org/},
	abstract = {A fast, consistent tool for working with data frame
    like objects, both in memory and out of memory.},
	urldate = {2022-03-17},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/SQLDTB9C/dplyr.tidyverse.org.html:text/html},
}

@online{noauthor_tidy_nodate,
	title = {Tidy Messy Data},
	url = {https://tidyr.tidyverse.org/},
	abstract = {Tools to help to create tidy data, where each column is a
    variable, each row is an observation, and each cell contains a single
    value.  tidyr contains tools for changing the shape (pivoting) and
    hierarchy (nesting and unnesting) of a dataset, turning deeply
    nested lists into rectangular data frames (rectangling), and
    extracting values out of string columns. It also includes tools for
    working with missing values (both implicit and explicit).},
	urldate = {2022-03-17},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/ZI6Z6PPQ/tidyr.tidyverse.org.html:text/html},
}

@inproceedings{zhang_automatically_2013,
	title = {Automatically synthesizing sql queries from input-output examples},
	pages = {224--234},
	booktitle = {2013 28th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Zhang, Sai and Sun, Yuyin},
	date = {2013},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/CCCM5RVH/6693082.html:text/html},
}

@article{gupta_synthesize_2020,
	title = {Synthesize, execute and debug: Learning to repair for neural program synthesis},
	volume = {33},
	shorttitle = {Synthesize, execute and debug},
	pages = {17685--17695},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Gupta, Kavi and Christensen, Peter Ebert and Chen, Xinyun and Song, Dawn},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/WC2NMZIE/Gupta et al. - 2020 - Synthesize, execute and debug Learning to repair .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/N5GZMG7K/cd0f74b5955dc87fd0605745c4b49ee8-Abstract.html:text/html},
}

@article{kostenetskiy_hpc_2021,
	title = {{HPC} Resources of the Higher School of Economics},
	volume = {1740},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1740/1/012050},
	doi = {10.1088/1742-6596/1740/1/012050},
	abstract = {The National Research University Higher School of Economics launched its {HPC} cluster and created a new division named the Supercomputer Simulation Unit. Now the university {HPC} cluster occupies seventh place in rating the most powerful computers of the {CIS} {TOP}50. The {HPC} cluster uses to solve machine learning problems, population genomics, hydrodynamics, atomistic and continuous modeling in physics, generative probabilistic models, financial row forecasting algorithms, and other actual problems. Paper describes the {HSE} {HPC} resources and experience of their use for scientific and educational tasks.},
	pages = {012050},
	number = {1},
	journaltitle = {Journal of Physics: Conference Series},
	shortjournal = {J. Phys.: Conf. Ser.},
	author = {Kostenetskiy, P. S. and Chulkevich, R. A. and Kozyrev, V. I.},
	urldate = {2022-03-21},
	date = {2021-01},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
	file = {IOP Full Text PDF:/Users/germanarutunov/Zotero/storage/WZJZEZSW/Kostenetskiy et al. - 2021 - HPC Resources of the Higher School of Economics.pdf:application/pdf},
}

@inproceedings{chebaro_combining_2010,
	title = {Combining static analysis and test generation for C program debugging},
	pages = {94--100},
	booktitle = {International Conference on Tests and Proofs},
	publisher = {Springer},
	author = {Chebaro, Omar and Kosmatov, Nikolai and Giorgetti, Alain and Julliand, Jacques},
	date = {2010},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/PG5P5UVB/Chebaro et al. - 2010 - Combining static analysis and test generation for .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/CZSFPW6G/978-3-642-13977-2_9.html:text/html},
}

@article{mukherjee_neural_2021,
	title = {Neural program generation modulo static analysis},
	volume = {34},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Mukherjee, Rohan and Wen, Yeming and Chaudhari, Dipak and Reps, Thomas and Chaudhuri, Swarat and Jermaine, Christopher},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/JA78UZ3R/Mukherjee et al. - 2021 - Neural program generation modulo static analysis.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/TM9NW64N/9e1a36515d6704d7eb7a30d783400e5d-Abstract.html:text/html},
}

@inproceedings{lebanidze_need_2008,
	title = {The Need for Fourth Generation Static Analysis Tools for Security–From Bugs to Flaws},
	booktitle = {Application Security Conference},
	author = {Lebanidze, Evgeny},
	date = {2008},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/PACFVTST/Lebanidze - 2008 - The Need for Fourth Generation Static Analysis Too.pdf:application/pdf},
}

@article{mcafee_utilizing_2012,
	title = {Utilizing static analysis and code generation to accelerate neural networks},
	journaltitle = {{arXiv} preprint {arXiv}:1206.6466},
	author = {{McAfee}, Lawrence and Olukotun, Kunle},
	date = {2012},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/3FXHYEDL/McAfee and Olukotun - 2012 - Utilizing static analysis and code generation to a.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/KYDD2HL9/1206.html:text/html},
}

@inproceedings{gupta_intelligent_2018,
	title = {Intelligent code reviews using deep learning},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining ({KDD}’18) Deep Learning Day},
	author = {Gupta, Anshul and Sundaresan, Neel},
	date = {2018},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/QIC8DYVI/Gupta and Sundaresan - 2018 - Intelligent code reviews using deep learning.pdf:application/pdf},
}

@article{sharma_survey_2021,
	title = {A Survey on Machine Learning Techniques for Source Code Analysis},
	journaltitle = {{arXiv} preprint {arXiv}:2110.09610},
	author = {Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Sarro, Federica},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/MXYBFK78/Sharma et al. - 2021 - A Survey on Machine Learning Techniques for Source.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/UDM7U6Y5/2110.html:text/html},
}

@article{sultanow_maschine_nodate,
	title = {Maschine Learning based Code Analysis for Software Quality Assurance},
	volume = {1},
	pages = {T2},
	journaltitle = {Transactions},
	author = {Sultanow, Eldar and Konopik, Stefan and Ullrich, André and Vladova, Gergana},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/QZZKEPIN/Sultanow et al. - Maschine Learning based Code Analysis for Software.pdf:application/pdf},
}

@article{__2017,
	title = {Обзор подходов к улучшению качества результатов статического анализа программ},
	volume = {29},
	rights = {Авторы, публикующие в данном журнале, соглашаются со следующим:   Авторы сохраняют за собой авторские права на работу и предоставляют журналу право первой публикации работы на условиях лицензии  Creative Commons Attribution License , которая позволяет другим распространять данную работу с обязательным сохранением ссылок на авторов оригинальной работы и оригинальную публикацию в этом журнале.  Авторы сохраняют право заключать отдельные контрактные договорённости, касающиеся не-эксклюзивного распространения версии работы в опубликованном здесь виде (например, размещение ее в институтском хранилище, публикацию в книге), со ссылкой на ее оригинальную публикацию в этом журнале.  Авторы имеют право размещать их работу в сети Интернет (например в институтском хранилище или персональном сайте) до и во время процесса рассмотрения ее данным журналом, так как это может привести к продуктивному обсуждению и большему количеству ссылок на данную работу (См.  The Effect of Open Access ).},
	issn = {2220-6426},
	url = {https://ispranproceedings.elpub.ru/jour/article/view/279},
	doi = {10.15514/ISPRAS-2017-29(3)-6},
	abstract = {Научный рецензируемый журнал, список ВАК},
	pages = {75--98},
	number = {3},
	journaltitle = {Труды Института системного программирования РАН},
	author = {Герасимов, А. Ю.},
	urldate = {2022-03-25},
	date = {2017},
	langid = {russian},
	note = {Number: 3},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/DDIWHHQ6/Герасимов - 2017 - Обзор подходов к улучшению качества результатов ст.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/HL3HNIQ6/279.html:text/html},
}

@article{khaliq_artificial_2022,
	title = {Artificial Intelligence in Software Testing: Impact, Problems, Challenges and Prospect},
	shorttitle = {Artificial Intelligence in Software Testing},
	journaltitle = {{arXiv} preprint {arXiv}:2201.05371},
	author = {Khaliq, Zubair and Farooq, Sheikh Umar and Khan, Dawood Ashraf},
	date = {2022},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/RHH8RBBI/Khaliq et al. - 2022 - Artificial Intelligence in Software Testing Impac.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/7ALNBSIF/2201.html:text/html},
}

@inproceedings{ansari_constructing_2017,
	title = {Constructing test cases using natural language processing},
	pages = {95--99},
	booktitle = {2017 Third International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics ({AEEICB})},
	publisher = {{IEEE}},
	author = {Ansari, Ahlam and Shagufta, Mirza Baig and Fatima, Ansari Sadaf and Tehreem, Shaikh},
	date = {2017},
	file = {Ansari et al. - 2017 - Constructing test cases using natural language pro.pdf:/Users/germanarutunov/Zotero/storage/HLN3AH7A/Ansari et al. - 2017 - Constructing test cases using natural language pro.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/J39MAJWR/7972390.html:text/html},
}

@thesis{saes_unit_2018,
	title = {Unit test generation using machine learning},
	institution = {Master’s thesis. University of Amsterdam, the Netherlands. Available: http …},
	type = {phdthesis},
	author = {Saes, Laurence},
	date = {2018},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/KPY2HFFB/Saes - 2018 - Unit test generation using machine learning.pdf:application/pdf},
}

@online{noauthor_deepcode_2019,
	title = {{DeepCode} learns from {GitHub} project data to give developers {AI}-powered code reviews},
	url = {https://venturebeat.com/2019/08/06/deepcode-learns-from-github-project-data-to-give-developers-ai-powered-code-reviews/},
	abstract = {{DeepCode} is an {AI}-powered code review system for developers, with machine learning systems trained on billions of lines of code from open source projects.},
	titleaddon = {{VentureBeat}},
	urldate = {2022-04-06},
	date = {2019-08-06},
	langid = {american},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/PHB5AEYY/deepcode-learns-from-github-project-data-to-give-developers-ai-powered-code-reviews.html:text/html},
}

@online{noauthor_aws_nodate,
	title = {{AWS} announces Amazon {CodeGuru} for automated code reviews and application performance recommendations},
	url = {https://aws.amazon.com/about-aws/whats-new/2019/12/aws-announces-amazon-codeguru-for-automated-code-reviews-and-application-performance-recommendations/},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2022-04-06},
	langid = {american},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/BG9Q64QD/aws-announces-amazon-codeguru-for-automated-code-reviews-and-application-performance-recommenda.html:text/html},
}

@software{noauthor_infer_2022,
	title = {Infer},
	rights = {{MIT}},
	url = {https://github.com/facebook/infer},
	abstract = {A static analyzer for Java, C, C++, and Objective-C},
	publisher = {Meta},
	urldate = {2022-04-06},
	date = {2022-04-06},
	note = {original-date: 2015-01-26T11:19:13Z},
	keywords = {c, code-quality, cpp, java, objective-c, static-analysis, static-code-analysis},
}

@inproceedings{marginean_sapfix_2019,
	title = {Sapfix: Automated end-to-end repair at scale},
	shorttitle = {Sapfix},
	pages = {269--278},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	publisher = {{IEEE}},
	author = {Marginean, Alexandru and Bader, Johannes and Chandra, Satish and Harman, Mark and Jia, Yue and Mao, Ke and Mols, Alexander and Scott, Andrew},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/H9RY3FHN/Marginean et al. - 2019 - Sapfix Automated end-to-end repair at scale.pdf:application/pdf},
}

@online{noauthor_embold_nodate,
	title = {Embold: Static Code Analyzer Uses {AI} To Help Developers Analyze And Improve Their Code - {MarkTechPost}},
	url = {https://www.marktechpost.com/2020/11/14/embold-static-code-analyzer-uses-ai-to-help-developers-analyze-and-improve-their-code/},
	urldate = {2022-04-06},
	file = {Embold\: Static Code Analyzer Uses AI To Help Developers Analyze And Improve Their Code - MarkTechPost:/Users/germanarutunov/Zotero/storage/GH3RHM4A/embold-static-code-analyzer-uses-ai-to-help-developers-analyze-and-improve-their-code.html:text/html},
}

@article{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	pages = {9},
	number = {8},
	journaltitle = {{OpenAI} blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/DQHI4QCW/Radford et al. - 2019 - Language models are unsupervised multitask learner.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	pages = {1877--1901},
	journaltitle = {Advances in neural information processing systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/XJWHNDP6/Brown et al. - 2020 - Language models are few-shot learners.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/5ELAIFB2/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html:text/html},
}

@article{wang_mconala_2022,
	title = {{MCoNaLa}: A Benchmark for Code Generation from Multiple Natural Languages},
	shorttitle = {{MCoNaLa}},
	journaltitle = {{arXiv} preprint {arXiv}:2203.08388},
	author = {Wang, Zhiruo and Cuenca, Grace and Zhou, Shuyan and Xu, Frank F. and Neubig, Graham},
	date = {2022},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/MRRG2WD8/Wang et al. - 2022 - MCoNaLa A Benchmark for Code Generation from Mult.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/BUHZ7IN9/2203.html:text/html},
}

@article{radford_learning_2017,
	title = {Learning to Generate Reviews and Discovering Sentiment},
	url = {https://arxiv.org/abs/1704.01444v2},
	doi = {10.48550/arXiv.1704.01444},
	abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
	author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
	urldate = {2022-04-13},
	date = {2017-04-05},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/GLWY4DZX/Radford et al. - 2017 - Learning to Generate Reviews and Discovering Senti.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/EGYBRNQG/1704.html:text/html},
}

@article{perez_automatic_2021,
	title = {Automatic Code Generation using Pre-Trained Language Models},
	journaltitle = {{arXiv} preprint {arXiv}:2102.10535},
	author = {Perez, Luis and Ottens, Lizi and Viswanathan, Sudharshan},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/CTNEBGXP/Perez et al. - 2021 - Automatic Code Generation using Pre-Trained Langua.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/YBL2WGYK/2102.html:text/html},
}

@article{xu_systematic_2022,
	title = {A Systematic Evaluation of Large Language Models of Code},
	journaltitle = {{arXiv} preprint {arXiv}:2202.13169},
	author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
	date = {2022},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/3M5IMIQM/Xu et al. - 2022 - A Systematic Evaluation of Large Language Models o.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/UHH5KY78/2202.html:text/html},
}

@software{hellendoorn_vhellendoorniclr20-great_2020,
	title = {{VHellendoorn}/{ICLR}20-Great: Working Version},
	rights = {Open Access},
	url = {https://zenodo.org/record/3668323},
	shorttitle = {{VHellendoorn}/{ICLR}20-Great},
	abstract = {Models of code can learn distributed representations of a program’s syntax and semantics to predict many non-trivial properties of a program. Recent state-ofthe-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g., data-ﬂow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers ({GREAT} for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identiﬁcation, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a graph-based model that already improves upon the prior state-of-the-art for this task by 20\%, we show that our proposed hybrid models improve an additional 10–15\%, while training both faster and using fewer parameters.},
	version = {v0.2},
	publisher = {Zenodo},
	author = {Hellendoorn, Vincent},
	urldate = {2022-04-13},
	date = {2020-07-21},
	doi = {10.5281/ZENODO.3668323},
}

@article{hellendoorn_global_2020,
	title = {{GLOBAL} {RELATIONAL} {MODELS} {OF} {SOURCE} {CODE}},
	abstract = {Models of code can learn distributed representations of a program’s syntax and semantics to predict many non-trivial properties of a program. Recent state-ofthe-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g., data-ﬂow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers ({GREAT} for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identiﬁcation, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a graph-based model that already improves upon the prior state-of-the-art for this task by 20\%, we show that our proposed hybrid models improve an additional 10–15\%, while training both faster and using fewer parameters.},
	pages = {12},
	author = {Hellendoorn, Vincent J and Maniatis, Petros and Singh, Rishabh and Sutton, Charles and Bieber, David},
	date = {2020},
	langid = {english},
	file = {Hellendoorn et al. - 2020 - GLOBAL RELATIONAL MODELS OF SOURCE CODE.pdf:/Users/germanarutunov/Zotero/storage/9UTMFCCG/Hellendoorn et al. - 2020 - GLOBAL RELATIONAL MODELS OF SOURCE CODE.pdf:application/pdf},
}

@inproceedings{hellendoorn_global_2019,
	title = {Global relational models of source code},
	booktitle = {International conference on learning representations},
	author = {Hellendoorn, Vincent J. and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/Y2NHKSV5/Hellendoorn et al. - 2019 - Global relational models of source code.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/6KLEQCWG/forum.html:text/html},
}

@inproceedings{van_stegeren_fine-tuning_2021,
	title = {Fine-tuning {GPT}-2 on annotated {RPG} quests for {NPC} dialogue generation},
	pages = {1--8},
	booktitle = {The 16th International Conference on the Foundations of Digital Games ({FDG}) 2021},
	author = {van Stegeren, Judith and Myśliwiec, Jakub},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/HMBQNPNZ/3472538.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/9KHMD7VS/3472538.html:text/html},
}

@online{noauthor_lee_nodate,
	title = {Lee: Patent claim generation by fine-tuning {OpenAI} {GPT}-2 - Google Scholar},
	url = {https://scholar.google.com/scholar_lookup?oi=gsb80&title=Patent%20Claim%20Generation%20by%20Fine-Tuning%20OpenAI%20GPT-2&author=Lee%2C%20Jieh-Sheng&author=Hsiang%2C%20Jieh&year=2019%2F07%2F01&online_date=2019%2F07%2F01&arxiv_id=1907.02052&doi=10.48550%2FarXiv.1907.02052&lookup=0&hl=en},
	urldate = {2022-04-19},
}

@misc{ressmeyer_deep_2019,
	title = {“Deep faking” political twitter using transfe r learning and {GPT}-2},
	publisher = {Stanford, {CA}, {USA}: Working paper Stanford University},
	author = {Ressmeyer, Ryan and Masling, Sam and Liao, Madeline},
	date = {2019},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/ME7NWGYA/Ressmeyer et al. - 2019 - “Deep faking” political twitter using transfe r le.pdf:application/pdf},
}

@article{lee_patent_2020,
	title = {Patent claim generation by fine-tuning {OpenAI} {GPT}-2},
	volume = {62},
	pages = {101983},
	journaltitle = {World Patent Information},
	author = {Lee, Jieh-Sheng and Hsiang, Jieh},
	date = {2020},
	note = {Publisher: Elsevier},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/ANPR2X4Q/S0172219019300766.html:text/html},
}

@inproceedings{liu_chinese_2021,
	title = {Chinese Judicial Summarising Based on Short Sentence Extraction and {GPT}-2},
	pages = {376--393},
	booktitle = {International Conference on Knowledge Science, Engineering and Management},
	publisher = {Springer},
	author = {Liu, Jie and Wu, Jiaye and Luo, Xudong},
	date = {2021},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/92E2BQB4/978-3-030-82147-0_31.html:text/html},
}

@article{beheitt_automatic_2022,
	title = {Automatic Arabic Poem Generation with {GPT}-2},
	author = {Beheitt, Mohamed El Ghaly and Hmida, Moez Ben Haj},
	date = {2022},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/755I9RHB/Beheitt and Hmida - 2022 - Automatic Arabic Poem Generation with GPT-2.pdf:application/pdf},
}

@article{paik_improving_2021,
	title = {Improving Text-to-Code Generation with Features of Code Graph on {GPT}-2},
	volume = {10},
	pages = {2706},
	number = {21},
	journaltitle = {Electronics},
	author = {Paik, Incheon and Wang, Jun-Wei},
	date = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/ZKKIGHGI/htm.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/KRS2E6K5/2706.html:text/html},
}

@article{narasimhan_cgems_2021,
	title = {{CGEMs}: A Metric Model for Automatic Code Generation using {GPT}-3},
	shorttitle = {{CGEMs}},
	journaltitle = {{arXiv} preprint {arXiv}:2108.10168},
	author = {Narasimhan, Aishwarya and Rao, Krishna Prasad Agara Venkatesha},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/TKMDWT7B/Narasimhan and Rao - 2021 - CGEMs A Metric Model for Automatic Code Generatio.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/7JTAB2RQ/2108.html:text/html},
}

@article{perez_automatic_2021-1,
	title = {Automatic Code Generation using Pre-Trained Language Models},
	journaltitle = {{arXiv} preprint {arXiv}:2102.10535},
	author = {Perez, Luis and Ottens, Lizi and Viswanathan, Sudharshan},
	date = {2021},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/KTRLM245/Perez et al. - 2021 - Automatic Code Generation using Pre-Trained Langua.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/7DI7MTCR/2102.html:text/html},
}

@article{church__1937,
	title = {A. M. Turing. On computable numbers, with an application to the Entscheidungs problcm. Proceedings of the London Mathematical Society, 2 s. vol. 42 (1936–1937), pp. 230–265.},
	volume = {2},
	issn = {1943-5886},
	url = {https://www.cambridge.org/core/article/m-turing-on-computable-numbers-with-an-application-to-the-entscheidungs-problcm-proceedings-of-the-london-mathematical-society-2-s-vol-42-19361937-pp-230265/4DFCA89035F7F7C5BF4DB5129B8BB09E},
	doi = {10.1017/S002248120003958X},
	pages = {42--43},
	number = {1},
	journaltitle = {Journal of Symbolic Logic},
	author = {Church, Alonzo},
	date = {1937},
}

@article{turing_computable_1936,
	title = {On computable numbers, with an application to the Entscheidungsproblem},
	volume = {58},
	pages = {5},
	number = {345},
	journaltitle = {J. of Math},
	author = {Turing, Alan Mathison},
	date = {1936},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/UZIBZUUG/Turing - 1936 - On computable numbers, with an application to the .pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/6WLMF9NG/4DFCA89035F7F7C5BF4DB5129B8BB09E.html:text/html},
}

@misc{wang_mesh-transformer-jax_2021,
	title = {Mesh-Transformer-{JAX}: Model-Parallel Implementation of Transformer Language Model with {JAX}},
	url = {https://github.com/kingoflolz/mesh-transformer-jax},
	author = {Wang, Ben},
	urldate = {2022-04-20},
	date = {2021-05},
}

@book{manning_foundations_1999,
	location = {Cambridge, {MA}, {USA}},
	title = {Foundations of statistical natural language processing},
	isbn = {978-0-262-13360-9},
	pagetotal = {680},
	publisher = {{MIT} Press},
	author = {Manning, Christopher D. and Schütze, Hinrich},
	date = {1999},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/M6NYZ77B/books.html:text/html},
}

@inproceedings{hamilton_inductive_2017-1,
	location = {Red Hook, {NY}, {USA}},
	title = {Inductive representation learning on large graphs},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present {GraphSAGE}, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	pages = {1025--1035},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2022-04-19},
	date = {2017-12-04},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/3C6PF92H/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf},
}

@article{nijkamp_conversational_2022,
	title = {A Conversational Paradigm for Program Synthesis},
	journaltitle = {{arXiv} preprint {arXiv}:2203.13474},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	date = {2022},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/7LZ5VNR9/Nijkamp et al. - 2022 - A Conversational Paradigm for Program Synthesis.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/EIMQDMDU/2203.html:text/html},
}

@article{schick_its_2021,
	title = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
	url = {http://arxiv.org/abs/2009.07118},
	shorttitle = {It's Not Just Size That Matters},
	abstract = {When scaled to hundreds of billions of parameters, pretrained language models such as {GPT}-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to {GPT}-3 can be obtained with language models that are much "greener" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
	journaltitle = {{arXiv}:2009.07118 [cs]},
	author = {Schick, Timo and Schütze, Hinrich},
	urldate = {2022-04-21},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2009.07118},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/2QJKPUU9/Schick and Schütze - 2021 - It's Not Just Size That Matters Small Language Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/KUS7I4U9/2009.html:text/html},
}

@article{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	journaltitle = {{arXiv}:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2022-04-21},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/KDEKXNRF/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/ELCSFTLU/2010.html:text/html},
}

@article{wei_finetuned_2022,
	title = {Finetuned Language Models Are Zero-Shot Learners},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 {NLP} tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call {FLAN}, on unseen task types. {FLAN} substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B {GPT}-3 on 20 of 25 tasks that we evaluate. {FLAN} even outperforms few-shot {GPT}-3 by a large margin on {ANLI}, {RTE}, {BoolQ}, {AI}2-{ARC}, {OpenbookQA}, and {StoryCloze}. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	journaltitle = {{arXiv}:2109.01652 [cs]},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	urldate = {2022-04-21},
	date = {2022-02-08},
	eprinttype = {arxiv},
	eprint = {2109.01652},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/A4KACXL8/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/5YB5L2TV/2109.html:text/html},
}

@inproceedings{chintagunta_medically_2021,
	title = {Medically Aware {GPT}-3 as a Data Generator for Medical Dialogue Summarization},
	url = {https://proceedings.mlr.press/v149/chintagunta21a.html},
	abstract = {In medical dialogue summarization, summaries must be coherent and must capture all the medically relevant information in the dialogue. However, learning effective models for summarization require large amounts of labeled data which is especially hard to obtain. We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information. We utilize {GPT}-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples (∼30x) leveraging low-shot learning and an ensemble method. In detailed experiments, we show that this approach produces high quality training data that can further be combined with human labeled data to get summaries that are strongly preferable to those produced by models trained on human data alone both in terms of medical accuracy and coherency.},
	eventtitle = {Machine Learning for Healthcare Conference},
	pages = {354--372},
	booktitle = {Proceedings of the 6th Machine Learning for Healthcare Conference},
	publisher = {{PMLR}},
	author = {Chintagunta, Bharath and Katariya, Namit and Amatriain, Xavier and Kannan, Anitha},
	urldate = {2022-04-21},
	date = {2021-10-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/6D7V5XWQ/Chintagunta et al. - 2021 - Medically Aware GPT-3 as a Data Generator for Medi.pdf:application/pdf},
}

@article{bajaj_muce_2022,
	title = {{MUCE}: a multilingual use case model extractor using {GPT}-3},
	issn = {2511-2112},
	url = {https://doi.org/10.1007/s41870-022-00884-2},
	doi = {10.1007/s41870-022-00884-2},
	shorttitle = {{MUCE}},
	abstract = {During the software requirement specification, the use case-based approach is used for requirements elicitation and analysis, for object-oriented software development. For identifying the use cases, traditionally, manual approach is used. Though some researchers have used semi-automated {NLP} techniques but it requires each word in a sentence to be tagged with its part-of-speech. In this paper, we exploit Generative Pre-trained Transformer-3 ({GPT}-3) for identifying the use case model by parsing the given software requirements. {GPT}-3 is an autoregressive language model that uses deep learning to produce human-like text. It is a language prediction model that can be tuned with Few-Shot learning to extract functional requirements in a multilingual setting. We present {MUCE}, a Multilingual Use Case model Extractor tool for the extraction of use cases, actors and their relationships from the given users’ software requirements. Further, not all requirement documents are English-centric and might be written in some native language. In such scenario, an automated, multilingual tool for structuring the use case model will really be a boon. The benefit of such an automated tool is that it helps in extracting use cases model as requirements specification quickly; overcoming the worries involved in manual ways. This tool can also prove to be a feedback to improve and fine-tune functional requirements so as to make them more complete. We evaluated the performance of our tool using Confusion Matrix. We authenticate our findings by validating various user requirements case studies written in different languages particularly, English, Spanish and French. Our tool performs better than existing state-of-the-art use case model extraction techniques. Our research endeavor will assist software designers and architects to generate actors and use cases instantaneously from the given users’ software requirement documents, and save their time.},
	journaltitle = {International Journal of Information Technology},
	shortjournal = {Int. j. inf. tecnol.},
	author = {Bajaj, Deepali and Goel, Anita and Gupta, S. C. and Batra, Hunar},
	urldate = {2022-04-21},
	date = {2022-02-17},
	langid = {english},
	keywords = {Autoregressive language model, Generative Pre-trained Transformer-3 ({GPT}-3), Natural language processing, Pre-trained deep learning model, Self-attention, Software requirement specification ({SRS})},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/UXKEEVC8/Bajaj et al. - 2022 - MUCE a multilingual use case model extractor usin.pdf:application/pdf},
}

@article{gao_pile_2020,
	title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
	url = {https://arxiv.org/abs/2101.00027v1},
	doi = {10.48550/arXiv.2101.00027},
	shorttitle = {The Pile},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 {GiB} English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of {GPT}-2 and {GPT}-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw {CC} and {CC}-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	urldate = {2022-04-22},
	date = {2020-12-31},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/NAS5SU3Q/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/UB7K85FB/2101.html:text/html},
}

@online{woolf_fun_2021,
	title = {Fun and Dystopia With {AI}-Based Code Generation Using {GPT}-J-6B},
	url = {https://minimaxir.com/2021/06/gpt-j-6b/},
	abstract = {At the least, {AI}-generated code is much more readable than the average human's.},
	titleaddon = {Max Woolf's Blog},
	author = {Woolf, Max},
	urldate = {2022-04-22},
	date = {2021-06-14},
	langid = {english},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/BEZ7DIP4/gpt-j-6b.html:text/html},
}

@online{novet_microsoft_2021,
	title = {Microsoft and {OpenAI} have a new A.I. tool that will give coding suggestions to software developers},
	url = {https://www.cnbc.com/2021/06/29/microsoft-github-copilot-ai-offers-coding-suggestions.html},
	abstract = {For decades researchers have tried to get programs to write programs. Microsoft and {OpenAI} are drawing on vast cloud computing power and extensive source code.},
	titleaddon = {{CNBC}},
	author = {Novet, Jordan},
	urldate = {2022-04-22},
	date = {2021-06-29},
	langid = {english},
	note = {Section: Technology},
	file = {Snapshot:/Users/germanarutunov/Zotero/storage/X77R8W37/microsoft-github-copilot-ai-offers-coding-suggestions.html:text/html},
}

@article{dabre_softmax_2020,
	title = {Softmax Tempering for Training Neural Machine Translation Models},
	journaltitle = {{arXiv} preprint {arXiv}:2009.09372},
	author = {Dabre, Raj and Fujita, Atsushi},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/FWXVZKVA/Dabre and Fujita - 2020 - Softmax Tempering for Training Neural Machine Tran.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/28TJUV9U/2009.html:text/html},
}

@article{pang_text_2020,
	title = {Text generation by learning from demonstrations},
	journaltitle = {{arXiv} preprint {arXiv}:2009.07839},
	author = {Pang, Richard Yuanzhe and He, He},
	date = {2020},
	file = {Full Text:/Users/germanarutunov/Zotero/storage/D5DAXDW7/Pang and He - 2020 - Text generation by learning from demonstrations.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/TV2MT7V3/2009.html:text/html},
}

@article{black_gpt-neox-20b_2022,
	title = {{GPT}-{NeoX}-20B: An Open-Source Autoregressive Language Model},
	url = {http://arxiv.org/abs/2204.06745},
	shorttitle = {{GPT}-{NeoX}-20B},
	abstract = {We introduce {GPT}-{NeoX}-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe {\textbackslash}model\{\}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that {GPT}-{NeoX}-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized {GPT}-3 and {FairSeq} models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/{EleutherAI}/gpt-neox.},
	journaltitle = {{arXiv}:2204.06745 [cs]},
	author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and {McDonell}, Kyle and Phang, Jason and Pieler, Michael and Prashanth, {USVSN} Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
	urldate = {2022-04-28},
	date = {2022-04-14},
	eprinttype = {arxiv},
	eprint = {2204.06745},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/NSXT848X/Black et al. - 2022 - GPT-NeoX-20B An Open-Source Autoregressive Langua.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/PST8UL8H/2204.html:text/html;Full Text:/Users/germanarutunov/Zotero/storage/3ERRVXH8/Black et al. - GPT-NeoX-20B An Open-Source Autoregressive Langua.pdf:application/pdf},
}

@inproceedings{nie_adversarial_2020,
	location = {Online},
	title = {Adversarial {NLI}: A New Benchmark for Natural Language Understanding},
	url = {https://aclanthology.org/2020.acl-main.441},
	doi = {10.18653/v1/2020.acl-main.441},
	shorttitle = {Adversarial {NLI}},
	abstract = {We introduce a new large-scale {NLI} benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular {NLI} benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for {NLU}, rather than a static benchmark that will quickly saturate.},
	eventtitle = {{ACL} 2020},
	pages = {4885--4901},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	urldate = {2022-04-28},
	date = {2020-07},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/N7X5YCAA/Nie et al. - 2020 - Adversarial NLI A New Benchmark for Natural Langu.pdf:application/pdf},
}

@inproceedings{vilares_head-qa_2019,
	location = {Florence, Italy},
	title = {{HEAD}-{QA}: A Healthcare Dataset for Complex Reasoning},
	url = {https://aclanthology.org/P19-1092},
	doi = {10.18653/v1/P19-1092},
	shorttitle = {{HEAD}-{QA}},
	abstract = {We present {HEAD}-{QA}, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) {HEAD}-{QA} challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.},
	eventtitle = {{ACL} 2019},
	pages = {960--966},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Vilares, David and Gómez-Rodríguez, Carlos},
	urldate = {2022-04-28},
	date = {2019-07},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/B9J4ISVW/Vilares and Gómez-Rodríguez - 2019 - HEAD-QA A Healthcare Dataset for Complex Reasonin.pdf:application/pdf},
}

@inproceedings{zellers_hellaswag_2019,
	location = {Florence, Italy},
	title = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
	url = {https://aclanthology.org/P19-1472},
	doi = {10.18653/v1/P19-1472},
	shorttitle = {{HellaSwag}},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of {BERT}, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting {HellaSwag}, a new challenge dataset. Though its questions are trivial for humans ({\textbackslash}textgreater95\% accuracy), state-of-the-art models struggle ({\textbackslash}textless48\%). We achieve this via Adversarial Filtering ({AF}), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. {AF} proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical `Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of {HellaSwag}, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for {NLP} research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	eventtitle = {{ACL} 2019},
	pages = {4791--4800},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	urldate = {2022-04-28},
	date = {2019-07},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/4RP52MYE/Zellers et al. - 2019 - HellaSwag Can a Machine Really Finish Your Senten.pdf:application/pdf},
}

@article{clark_think_2018,
	title = {Think you have Solved Question Answering? Try {ARC}, the {AI}2 Reasoning Challenge},
	url = {https://arxiv.org/abs/1803.05457v1},
	doi = {10.48550/arXiv.1803.05457},
	shorttitle = {Think you have Solved Question Answering?},
	abstract = {We present a new question set, text corpus, and baselines assembled to encourage {AI} research in advanced question answering. Together, these constitute the {AI}2 Reasoning Challenge ({ARC}), which requires far more powerful knowledge and reasoning than previous challenges such as {SQuAD} or {SNLI}. The {ARC} question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the {SQuAD} and {SNLI} tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the {ARC} Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose {ARC} as a challenge to the community.},
	author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
	urldate = {2022-04-28},
	date = {2018-03-14},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/WL5FKGAE/Clark et al. - 2018 - Think you have Solved Question Answering Try ARC,.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/FW7IATY5/1803.html:text/html},
}

@inproceedings{paperno_lambada_2016,
	location = {Berlin, Germany},
	title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
	url = {https://aclanthology.org/P16-1144},
	doi = {10.18653/v1/P16-1144},
	shorttitle = {The {LAMBADA} dataset},
	eventtitle = {{ACL} 2016},
	pages = {1525--1534},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Ngoc Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
	urldate = {2022-04-28},
	date = {2016-08},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/4WFMX6KY/Paperno et al. - 2016 - The LAMBADA dataset Word prediction requiring a b.pdf:application/pdf},
}

@inproceedings{liu_logiqa_2020,
	title = {{LogiQA}: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
	volume = {4},
	url = {https://www.ijcai.org/proceedings/2020/501},
	doi = {10.24963/ijcai.2020/501},
	shorttitle = {{LogiQA}},
	abstract = {Electronic proceedings of {IJCAI} 2020},
	eventtitle = {Twenty-Ninth International Joint Conference on Artificial Intelligence},
	pages = {3622--3628},
	author = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
	urldate = {2022-04-28},
	date = {2020-07-09},
	langid = {english},
	note = {{ISSN}: 1045-0823},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/RQE6CRCB/Liu et al. - 2020 - LogiQA A Challenge Dataset for Machine Reading Co.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/YC3CVWQK/501.html:text/html},
}

@inproceedings{mihaylov_can_2018,
	location = {Brussels, Belgium},
	title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
	url = {https://aclanthology.org/D18-1260},
	doi = {10.18653/v1/D18-1260},
	shorttitle = {Can a Suit of Armor Conduct Electricity?},
	abstract = {We present a new kind of question answering dataset, {OpenBookQA}, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing {QA} datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, {OpenBookQA} probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on {OpenBookQA} is close to 92\%, but many state-of-the-art pre-trained {QA} methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.},
	eventtitle = {{EMNLP} 2018},
	pages = {2381--2391},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
	urldate = {2022-04-28},
	date = {2018-10},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/Z66YD4TM/Mihaylov et al. - 2018 - Can a Suit of Armor Conduct Electricity A New Dat.pdf:application/pdf},
}

@article{bisk_piqa_2020,
	title = {{PIQA}: Reasoning about Physical Commonsense in Natural Language},
	volume = {34},
	rights = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6239},
	doi = {10.1609/aaai.v34i05.6239},
	shorttitle = {{PIQA}},
	abstract = {To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as {BERT}) have made progress on question answering over more abstract domains – such as news articles and encyclopedia entries, where text is plentiful – in more physical domains, text is inherently limited due to reporting bias. Can {AI} systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or {PIQA}. Though humans find the dataset easy (95\% accuracy), large pretrained models struggle (∼75\%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.},
	pages = {7432--7439},
	number = {5},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
	urldate = {2022-04-28},
	date = {2020-04-03},
	langid = {english},
	note = {Number: 05},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/DD8JSPV8/Bisk et al. - 2020 - PIQA Reasoning about Physical Commonsense in Natu.pdf:application/pdf},
}

@inproceedings{aroca-ouellette_prost_2021,
	location = {Online},
	title = {{PROST}: Physical Reasoning about Objects through Space and Time},
	url = {https://aclanthology.org/2021.findings-acl.404},
	doi = {10.18653/v1/2021.findings-acl.404},
	shorttitle = {{PROST}},
	eventtitle = {Findings 2021},
	pages = {4597--4608},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Aroca-Ouellette, Stéphane and Paik, Cory and Roncone, Alessandro and Kann, Katharina},
	urldate = {2022-04-28},
	date = {2021-08},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/JG5WBIFC/Aroca-Ouellette et al. - 2021 - PROST Physical Reasoning about Objects through Sp.pdf:application/pdf},
}

@inproceedings{penas_qa4mre_2013,
	location = {Berlin, Heidelberg},
	title = {{QA}4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation},
	isbn = {978-3-642-40802-1},
	doi = {10.1007/978-3-642-40802-1_29},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{QA}4MRE 2011-2013},
	abstract = {This paper describes the methodology for testing the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. This was the attempt of the {QA}4MRE challenge which was run as a Lab at {CLEF} 2011–2013. The traditional {QA} task was replaced by a new Machine Reading task, whose intention was to ask questions that required a deep knowledge of individual short texts and in which systems were required to choose one answer, by analysing the corresponding test document in conjunction with background text collections provided by the organization. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer’s disease, and Entrance Exams. This paper describes their motivation, their goals, their methodology for preparing the data sets, their background collections, their metrics used for the evaluation, and the lessons learned along these three years.},
	pages = {303--320},
	booktitle = {Information Access Evaluation. Multilinguality, Multimodality, and Visualization},
	publisher = {Springer},
	author = {Peñas, Anselmo and Hovy, Eduard and Forner, Pamela and Rodrigo, Álvaro and Sutcliffe, Richard and Morante, Roser},
	editor = {Forner, Pamela and Müller, Henning and Paredes, Roberto and Rosso, Paolo and Stein, Benno},
	date = {2013},
	langid = {english},
	keywords = {Entrance Exam, Incorrect Answer, Question Answering, Question Answering System, Test Document},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/433NJEKA/Peñas et al. - 2013 - QA4MRE 2011-2013 Overview of Question Answering f.pdf:application/pdf},
}

@inproceedings{welbl_crowdsourcing_2017,
	location = {Copenhagen, Denmark},
	title = {Crowdsourcing Multiple Choice Science Questions},
	url = {https://aclanthology.org/W17-4413},
	doi = {10.18653/v1/W17-4413},
	abstract = {We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled {SciQ}, a dataset of 13.7K multiple choice science exam questions. We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using {SciQ} as additional training data to existing questions, we observe accuracy improvements on real science exams.},
	pages = {94--106},
	booktitle = {Proceedings of the 3rd Workshop on Noisy User-generated Text},
	publisher = {Association for Computational Linguistics},
	author = {Welbl, Johannes and Liu, Nelson F. and Gardner, Matt},
	urldate = {2022-04-28},
	date = {2017-09},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/6GJJHGXN/Welbl et al. - 2017 - Crowdsourcing Multiple Choice Science Questions.pdf:application/pdf},
}

@inproceedings{joshi_triviaqa_2017,
	location = {Vancouver, Canada},
	title = {{TriviaQA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	url = {https://aclanthology.org/P17-1147},
	doi = {10.18653/v1/P17-1147},
	shorttitle = {{TriviaQA}},
	abstract = {We present {TriviaQA}, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. {TriviaQA} includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, {TriviaQA} (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on {SQuAD} reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that {TriviaQA} is a challenging testbed that is worth significant future study.},
	eventtitle = {{ACL} 2017},
	pages = {1601--1611},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
	urldate = {2022-04-28},
	date = {2017-07},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/72Q6LF35/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf:application/pdf},
}

@article{sakaguchi_winogrande_2021,
	title = {{WinoGrande}: an adversarial winograd schema challenge at scale},
	volume = {64},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3474381},
	doi = {10.1145/3474381},
	shorttitle = {{WinoGrande}},
	abstract = {Commonsense reasoning remains a major challenge in {AI}, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90\% accuracy on the Winograd Schema Challenge ({WSC}), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce {WinoGrande}, a large-scale dataset of 44k problems, inspired by the original {WSC}, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel {AFLITE} algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4\%-79.1\%) on {WINOGRANDE} compared to humans (94\%), confirming that the high performance on the original {WSC} was inflated by spurious biases in the dataset. Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of {WINOGRANDE} when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
	pages = {99--106},
	number = {9},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	urldate = {2022-04-28},
	date = {2021-08-24},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/Z6CTUP97/Sakaguchi et al. - 2021 - WinoGrande an adversarial winograd schema challen.pdf:application/pdf},
}

@inproceedings{wang_superglue_2019,
	title = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
	shorttitle = {{SuperGLUE}},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The {GLUE} benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present {SuperGLUE}, a new benchmark styled after {GLUE} with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. {SuperGLUE} is available at https://super.gluebenchmark.com.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	urldate = {2022-04-28},
	date = {2019},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/N637XAAS/Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf:application/pdf},
}

@article{hendrycks_measuring_2021-1,
	title = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
	url = {https://arxiv.org/abs/2103.03874v2},
	doi = {10.48550/arXiv.2103.03874},
	abstract = {Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce {MATH}, a new dataset of 12,500 challenging competition mathematics problems. Each problem in {MATH} has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on {MATH}, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on {MATH}, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving {MATH}. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	urldate = {2022-04-28},
	date = {2021-03-05},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/WSNTE4YQ/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/46QVKW2R/2103.html:text/html},
}

@article{hendrycks_measuring_2020,
	title = {Measuring Massive Multitask Language Understanding},
	url = {https://arxiv.org/abs/2009.03300v3},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, {US} history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest {GPT}-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	urldate = {2022-04-28},
	date = {2020-09-07},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/BGC3XTKQ/Hendrycks et al. - 2020 - Measuring Massive Multitask Language Understanding.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/BMNNXMZ8/2009.html:text/html},
}

@thesis{__2012,
	location = {Москва},
	title = {Современные методы статического и динамического анализа программ для автоматизации процессов повышения качества программного обеспечения},
	url = {https://dissercat.com/content/sovremennye-metody-staticheskogo-i-dinamicheskogo-analiza-programm-dlya-avtomatizatsii-prots},
	abstract = {Аветисян, Арутюн Ишханович. Современные методы статического и динамического анализа программ для автоматизации процессов повышения качества программного обеспечения: дис. доктор физико-математических наук: 05.13.11 - Математическое и программное обеспечение вычислительных машин, комплексов и компьютерных сетей. Москва. 2012. 271 с.},
	type = {доктор физико-математических наук},
	author = {Аветисян, Арутюн Ишханович},
	urldate = {2022-04-28},
	date = {2012},
	langid = {russian},
	note = {Pages: 271},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/986U2DHL/Аветисян - 2012 - Современные методы статического и динамического ан.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/H5WLCU2K/sovremennye-metody-staticheskogo-i-dinamicheskogo-analiza-programm-dlya-avtomatizatsii-prots.html:text/html},
}

@article{__2015,
	title = {Статический анализатор Svace как коллекция анализаторов разных уровней сложности},
	volume = {27},
	rights = {Авторы, публикующие в данном журнале, соглашаются со следующим:   Авторы сохраняют за собой авторские права на работу и предоставляют журналу право первой публикации работы на условиях лицензии  Creative Commons Attribution License , которая позволяет другим распространять данную работу с обязательным сохранением ссылок на авторов оригинальной работы и оригинальную публикацию в этом журнале.  Авторы сохраняют право заключать отдельные контрактные договорённости, касающиеся не-эксклюзивного распространения версии работы в опубликованном здесь виде (например, размещение ее в институтском хранилище, публикацию в книге), со ссылкой на ее оригинальную публикацию в этом журнале.  Авторы имеют право размещать их работу в сети Интернет (например в институтском хранилище или персональном сайте) до и во время процесса рассмотрения ее данным журналом, так как это может привести к продуктивному обсуждению и большему количеству ссылок на данную работу (См.  The Effect of Open Access ).},
	issn = {2220-6426},
	url = {https://ispranproceedings.elpub.ru/jour/article/view/917},
	doi = {10.15514/ISPRAS-2015-27(6)-8},
	abstract = {Научный рецензируемый журнал, список ВАК},
	pages = {111--134},
	number = {6},
	journaltitle = {Труды Института системного программирования РАН},
	author = {Бородин, А. Е. and Белеванцев, А. А.},
	urldate = {2022-04-28},
	date = {2015},
	langid = {russian},
	note = {Number: 6},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/H2L8583V/Бородин and Белеванцев - 2015 - Статический анализатор Svace как коллекция анализа.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/GX33FIGH/917.html:text/html},
}

@thesis{__2016,
	location = {ФГБУН Институт системного программирования им. В.П. Иванникова Российской академии наук},
	title = {Межпроцедурный контекстно-чувствительный статический анализ для поиска ошибок в исходном коде программ на языках Си и Си++},
	url = {https://dissercat.com/content/mezhprotsedurnyi-kontekstno-chuvstvitelnyi-staticheskii-analiz-dlya-poiska-oshibok-v-iskhodn},
	abstract = {Бородин Алексей Евгеньевич. Межпроцедурный контекстно-чувствительный статический анализ для поиска ошибок в исходном коде программ на языках Си и Си++: дис. кандидат наук: 05.13.11 - Математическое и программное обеспечение вычислительных машин, комплексов и компьютерных сетей. ФГБУН Институт системного программирования им. В.П. Иванникова Российской академии наук. 2016. 137 с.},
	type = {кандидат наук},
	author = {Евгеньевич, Бородин Алексей},
	urldate = {2022-04-28},
	date = {2016},
	langid = {russian},
	note = {Pages: 137},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/6T7P3CND/Евгеньевич - 2016 - Межпроцедурный контекстно-чувствительный статическ.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/84YXSJG9/mezhprotsedurnyi-kontekstno-chuvstvitelnyi-staticheskii-analiz-dlya-poiska-oshibok-v-iskhodn.html:text/html},
}

@article{gao_pile_2020-1,
	title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
	url = {https://arxiv.org/abs/2101.00027v1},
	doi = {10.48550/arXiv.2101.00027},
	shorttitle = {The Pile},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 {GiB} English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of {GPT}-2 and {GPT}-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw {CC} and {CC}-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	urldate = {2022-04-28},
	date = {2020-12-31},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/EXSW2D3S/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/HKFYW5A9/2101.html:text/html},
}

@article{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
	url = {http://arxiv.org/abs/1909.08053},
	shorttitle = {Megatron-{LM}},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native {PyTorch}. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 {GPUs}. We sustain 15.1 {PetaFLOPs} across the entire application with 76\% scaling efficiency when compared to a strong single {GPU} baseline that sustains 39 {TeraFLOPs}, which is 30\% of peak {FLOPs}. To demonstrate that large language models can further advance the state of the art ({SOTA}), we train an 8.3 billion parameter transformer language model similar to {GPT}-2 and a 3.9 billion parameter model similar to {BERT}. We show that careful attention to the placement of layer normalization in {BERT}-like models is critical to achieving increased performance as the model size grows. Using the {GPT}-2 model we achieve {SOTA} results on the {WikiText}103 (10.8 compared to {SOTA} perplexity of 15.8) and {LAMBADA} (66.5\% compared to {SOTA} accuracy of 63.2\%) datasets. Our {BERT} model achieves {SOTA} results on the {RACE} dataset (90.9\% compared to {SOTA} accuracy of 89.4\%).},
	journaltitle = {{arXiv}:1909.08053 [cs]},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and {LeGresley}, Patrick and Casper, Jared and Catanzaro, Bryan},
	urldate = {2022-04-28},
	date = {2020-03-13},
	eprinttype = {arxiv},
	eprint = {1909.08053},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/4YBLTJQJ/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/T59SPEQU/1909.html:text/html},
}

@article{harlap_pipedream_2018,
	title = {{PipeDream}: Fast and Efficient Pipeline Parallel {DNN} Training},
	url = {http://arxiv.org/abs/1806.03377},
	shorttitle = {{PipeDream}},
	abstract = {{PipeDream} is a Deep Neural Network({DNN}) training system for {GPUs} that parallelizes computation by pipelining execution across multiple machines. Its pipeline parallel computing model avoids the slowdowns faced by data-parallel training when large models and/or limited network bandwidth induce high communication-to-computation ratios. {PipeDream} reduces communication by up to 95\% for large {DNNs} relative to data-parallel training, and allows perfect overlap of communication and computation. {PipeDream} keeps all available {GPUs} productive by systematically partitioning {DNN} layers among them to balance work and minimize communication, versions model parameters for backward pass correctness, and schedules the forward and backward passes of different inputs in round-robin fashion to optimize "time to target accuracy". Experiments with five different {DNNs} on two different clusters show that {PipeDream} is up to 5x faster in time-to-accuracy compared to data-parallel training.},
	journaltitle = {{arXiv}:1806.03377 [cs]},
	author = {Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
	urldate = {2022-04-28},
	date = {2018-06-08},
	eprinttype = {arxiv},
	eprint = {1806.03377},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/M9Y2XS5U/Harlap et al. - 2018 - PipeDream Fast and Efficient Pipeline Parallel DN.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/LN78VZI7/1806.html:text/html},
}

@article{rajbhandari_zero_2020,
	title = {{ZeRO}: Memory Optimizations Toward Training Trillion Parameter Models},
	url = {http://arxiv.org/abs/1910.02054},
	shorttitle = {{ZeRO}},
	abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer ({ZeRO}), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. {ZeRO} eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: {ZeRO} has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate {ZeRO}: it trains large models of over 100B parameter with super-linear speedup on 400 {GPUs}, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, {ZeRO} can train large models of up to 13B parameters (e.g., larger than Megatron {GPT} 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of {ZeRO} to create the world's largest language model (Turing-{NLG}, 17B parameters) with record breaking accuracy.},
	journaltitle = {{arXiv}:1910.02054 [cs, stat]},
	author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	urldate = {2022-05-05},
	date = {2020-05-13},
	eprinttype = {arxiv},
	eprint = {1910.02054},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/757LQ34L/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillio.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/SDYQDZUI/1910.html:text/html},
}

@inproceedings{rasley_deepspeed_2020,
	location = {New York, {NY}, {USA}},
	title = {{DeepSpeed}: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3406703},
	doi = {10.1145/3394486.3406703},
	series = {{KDD} '20},
	shorttitle = {{DeepSpeed}},
	abstract = {Explore new techniques in Microsoft's open source library called {DeepSpeed}, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. {DeepSpeed} is compatible with {PyTorch}. One piece of our library, called {ZeRO}, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-{NLG}), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the {DeepSpeed} team to achieve the world fastest {BERT} pretraining record. The Zero Redundancy Optimizer ({ZeRO}) is a novel memory optimization technology for large-scale distributed deep learning. {ZeRO} can train deep learning models with over 100 billion parameters on the current generation of {GPU} clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. {DeepSpeed} brings state-of-the-art training techniques, such as {ZeRO}, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight {APIs} compatible with {PyTorch}. With just a few lines of code changes to your {PyTorch} model, you can leverage {DeepSpeed} to address underlying performance challenges and boost the speed and scale of your training.},
	pages = {3505--3506},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	urldate = {2022-05-05},
	date = {2020-08-23},
	keywords = {distributed deep learning, machine learning},
}

@article{hoffmann_training_2022,
	title = {Training Compute-Optimal Large Language Models},
	url = {https://arxiv.org/abs/2203.15556v1},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over {\textbackslash}nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, {\textbackslash}chinchilla, that uses the same compute budget as {\textbackslash}gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. {\textbackslash}chinchilla uniformly and significantly outperforms {\textbackslash}Gopher (280B), {GPT}-3 (175B), Jurassic-1 (178B), and Megatron-Turing {NLG} (530B) on a large range of downstream evaluation tasks. This also means that {\textbackslash}chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, {\textbackslash}chinchilla reaches a state-of-the-art average accuracy of 67.5{\textbackslash}\% on the {MMLU} benchmark, greater than a 7{\textbackslash}\% improvement over {\textbackslash}gopher.},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	urldate = {2022-05-20},
	date = {2022-03-29},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/VS98C9GH/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/9VZGTILQ/2203.html:text/html},
}

@article{reynolds_prompt_2021,
	title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
	url = {https://arxiv.org/abs/2102.07350v1},
	doi = {10.48550/arXiv.2102.07350},
	shorttitle = {Prompt Programming for Large Language Models},
	abstract = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using {GPT}-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
	author = {Reynolds, Laria and {McDonell}, Kyle},
	urldate = {2022-05-20},
	date = {2021-02-15},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/BTUTWY3U/Reynolds and McDonell - 2021 - Prompt Programming for Large Language Models Beyo.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/5SSN28NX/2102.html:text/html},
}

@inproceedings{yao_staqc_2018,
	title = {{StaQC}: A Systematically Mined Question-Code Dataset from Stack Overflow},
	url = {http://arxiv.org/abs/1803.09371},
	doi = {10.1145/3178876.3186081},
	shorttitle = {{StaQC}},
	abstract = {Stack Overflow ({SO}) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and {SQL} domain, our framework substantially outperforms heuristic methods with at least 15\% higher F1 and accuracy. Furthermore, we present {StaQC} (Stack Overflow Question-Code pairs), the largest dataset to date of {\textasciitilde}148K Python and {\textasciitilde}120K {SQL} question-code pairs, automatically mined from {SO} using our framework. Under various case studies, we demonstrate that {StaQC} can greatly help develop data-hungry models for associating natural language with programming language.},
	pages = {1693--1703},
	booktitle = {Proceedings of the 2018 World Wide Web Conference on World Wide Web - {WWW} '18},
	author = {Yao, Ziyu and Weld, Daniel S. and Chen, Wei-Peng and Sun, Huan},
	urldate = {2022-06-17},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1803.09371 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/WUZALZKS/Yao et al. - 2018 - StaQC A Systematically Mined Question-Code Datase.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/BZ4WDLPR/1803.html:text/html},
}

@online{yao_staqc_2022,
	title = {{StaQC}: A Systematically Mined Question-Code Dataset from Stack Overflow},
	url = {https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset},
	shorttitle = {{StaQC}},
	abstract = {{StaQC}: a systematically mined dataset containing around 148K Python and 120K {SQL} domain question-code pairs, as described in "{StaQC}: A Systematically Mined Question-Code Dataset from Stack Overflow" ({WWW}'18)},
	author = {Yao, Ziyu},
	urldate = {2022-06-17},
	date = {2022-06-17},
	note = {original-date: 2018-01-06T23:08:10Z},
}

@inproceedings{yin_learning_2018,
	location = {New York, {NY}, {USA}},
	title = {Learning to mine aligned code and natural language pairs from stack overflow},
	isbn = {978-1-4503-5716-6},
	url = {https://doi.org/10.1145/3196398.3196408},
	doi = {10.1145/3196398.3196408},
	series = {{MSR} '18},
	abstract = {For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language ({NL}) and code with fine-grained alignments. Stack Overflow ({SO}) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the {NL}-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from {SO} using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between {NL} and code using neural networks. These features are fed into a classifier that determines the quality of mined {NL}-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling {NL}-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.},
	pages = {476--486},
	booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
	publisher = {Association for Computing Machinery},
	author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
	urldate = {2022-06-17},
	date = {2018-05-28},
	file = {Submitted Version:/Users/germanarutunov/Zotero/storage/6GQYZ4BB/Yin et al. - 2018 - Learning to mine aligned code and natural language.pdf:application/pdf},
}

@online{caballero_description2code_2016,
	title = {Description2Code Dataset},
	url = {https://github.com/ethancaballero/description2code},
	author = {Caballero, Ethan and {OpenAI}, . and Sutskever, Ilya},
	urldate = {2022-06-17},
	date = {2016-08},
	doi = {10.5281/zenodo.5665051},
}

@misc{puri_codenet_2021,
	title = {{CodeNet}: A Large-Scale {AI} for Code Dataset for Learning a Diversity of Coding Tasks},
	url = {http://arxiv.org/abs/2105.12655},
	doi = {10.48550/arXiv.2105.12655},
	shorttitle = {{CodeNet}},
	abstract = {Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage {AI} techniques to improve software development efficiency. Thus, the fast-emerging research area of {AI} for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset {CodeNet}, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching {AI} to code. In addition to its large scale, {CodeNet} has a rich set of high-quality annotations to benchmark and help accelerate research in {AI} techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, {CodeNet} provides sample input and output test sets for 98.5\% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in {CodeNet} to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the {CodeNet} dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of {CodeNet} will offer unprecedented research opportunities at the intersection of {AI} and Software Engineering.},
	number = {{arXiv}:2105.12655},
	publisher = {{arXiv}},
	author = {Puri, Ruchir and Kung, David S. and Janssen, Geert and Zhang, Wei and Domeniconi, Giacomo and Zolotov, Vladimir and Dolby, Julian and Chen, Jie and Choudhury, Mihir and Decker, Lindsey and Thost, Veronika and Buratti, Luca and Pujar, Saurabh and Ramji, Shyam and Finkler, Ulrich and Malaika, Susan and Reiss, Frederick},
	urldate = {2022-06-17},
	date = {2021-08-29},
	eprinttype = {arxiv},
	eprint = {2105.12655 [cs]},
	note = {Number: {arXiv}:2105.12655},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/ZZEJI2ZI/Puri et al. - 2021 - CodeNet A Large-Scale AI for Code Dataset for Lea.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/CB9PK5HW/2105.html:text/html},
}

@online{ibm_project_2022,
	title = {Project {CodeNet}},
	rights = {Apache-2.0},
	url = {https://github.com/IBM/Project_CodeNet},
	abstract = {This repository is to support contributions for tools for the Project {CodeNet} dataset hosted in {DAX}},
	author = {{IBM}},
	urldate = {2022-06-17},
	date = {2022-06-15},
	note = {original-date: 2021-05-03T02:51:25Z},
}

@misc{barone_parallel_2017-1,
	title = {A parallel corpus of Python functions and documentation strings for automated code documentation and code generation},
	url = {http://arxiv.org/abs/1707.02275},
	doi = {10.48550/arXiv.1707.02275},
	abstract = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings ("docstrings") generated by scraping open source repositories on {GitHub}. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data. We release our datasets and processing scripts in order to stimulate research in these areas.},
	number = {{arXiv}:1707.02275},
	publisher = {{arXiv}},
	author = {Barone, Antonio Valerio Miceli and Sennrich, Rico},
	urldate = {2022-06-17},
	date = {2017-07-07},
	eprinttype = {arxiv},
	eprint = {1707.02275 [cs]},
	note = {Number: {arXiv}:1707.02275},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/JLL6A2WL/Barone and Sennrich - 2017 - A parallel corpus of Python functions and document.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/F8DLLXLY/1707.html:text/html},
}

@online{noauthor_code-docstring-corpus_2022,
	title = {code-docstring-corpus},
	url = {https://github.com/EdinburghNLP/code-docstring-corpus},
	abstract = {Preprocessed Python functions and docstrings for automated code documentation (code2doc) and automated code generation (doc2code) tasks.},
	urldate = {2022-06-17},
	date = {2022-06-17},
	note = {original-date: 2017-04-07T16:15:37Z},
	keywords = {code-generation, corpus, docstrings, documentation-generator, neural-machine-translation},
}

@inproceedings{raychev_probabilistic_2016,
	location = {New York, {NY}, {USA}},
	title = {Probabilistic model for code with decision trees},
	isbn = {978-1-4503-4444-9},
	url = {https://doi.org/10.1145/2983990.2984041},
	doi = {10.1145/2983990.2984041},
	series = {{OOPSLA} 2016},
	abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., {GitHub}) to make predictions about new programs (e.g., code completion, repair, etc). The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called {TGen}). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as {ID}3, but also to obtain new variants we refer to as {ID}3+ and E13, not previously explored and ones that outperform {ID}3 in prediction accuracy. Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of {JavaScript} and Python. Our experimental results indicate that Deep3 predicts elements of {JavaScript} and Python code with precision above 82\% and 69\%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
	pages = {731--747},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
	publisher = {Association for Computing Machinery},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
	urldate = {2022-06-17},
	date = {2016-10-19},
	keywords = {Code Completion, Decision Trees, Probabilistic Models of Code},
}

@article{sm_avdoshin_code_2022,
	title = {Code Analysis and Generation Methods Using Neural Networks: an Overview},
	volume = {28},
	doi = {10.17587/it.28.378-391},
	pages = {378--391},
	number = {7},
	journaltitle = {{INFORMATION} {TECHNOLOGIES}},
	author = {{S.M. Avdoshin} and {G.A. Arutyunov}},
	date = {2022},
}

@misc{dwivedi_generalization_2021,
	title = {A Generalization of Transformer Networks to Graphs},
	url = {http://arxiv.org/abs/2012.09699},
	doi = {10.48550/arXiv.2012.09699},
	abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing ({NLP}), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in {NLP}. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
	number = {{arXiv}:2012.09699},
	publisher = {{arXiv}},
	author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
	urldate = {2022-06-17},
	date = {2021-01-24},
	eprinttype = {arxiv},
	eprint = {2012.09699 [cs]},
	note = {Number: {arXiv}:2012.09699},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/Z8BTYAMQ/Dwivedi and Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/EJKMKBDH/2012.html:text/html},
}

@misc{kreuzer_rethinking_2021,
	title = {Rethinking Graph Transformers with Spectral Attention},
	url = {http://arxiv.org/abs/2106.03893},
	doi = {10.48550/arXiv.2106.03893},
	abstract = {In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the \${\textbackslash}textit\{Spectral Attention Network\}\$ ({SAN}), which uses a learned positional encoding ({LPE}) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This {LPE} is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most {GNNs}, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art {GNNs}, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.},
	number = {{arXiv}:2106.03893},
	publisher = {{arXiv}},
	author = {Kreuzer, Devin and Beaini, Dominique and Hamilton, William L. and Létourneau, Vincent and Tossou, Prudencio},
	urldate = {2022-06-17},
	date = {2021-10-27},
	eprinttype = {arxiv},
	eprint = {2106.03893 [cs]},
	note = {Number: {arXiv}:2106.03893},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/EP5DSM6R/Kreuzer et al. - 2021 - Rethinking Graph Transformers with Spectral Attent.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/FGNCF6S6/2106.html:text/html},
}

@online{noauthor_graph_2022,
	title = {Graph Transformer Architecture},
	rights = {{MIT}},
	url = {https://github.com/graphdeeplearning/graphtransformer},
	abstract = {Graph Transformer Architecture. Source code for "A Generalization of Transformer Networks to Graphs", {DLG}-{AAAI}'21.},
	urldate = {2022-06-17},
	date = {2022-06-17},
	note = {original-date: 2020-12-17T11:13:51Z},
	keywords = {graph-neural-networks, aaai, attention, graph-deep-learning, graph-transformer, transformer, transformer-networks, transformers},
}

@online{kreuzer_san_2022,
	title = {{SAN}},
	rights = {{MIT}},
	url = {https://github.com/DevinKreuzer/SAN},
	author = {Kreuzer, Devin},
	urldate = {2022-06-17},
	date = {2022-05-29},
	note = {original-date: 2021-05-27T18:36:55Z},
}

@online{noauthor_introducing_2021,
	title = {Introducing {GitHub} Copilot: your {AI} pair programmer},
	url = {https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/},
	shorttitle = {Introducing {GitHub} Copilot},
	abstract = {Today, we're launching a technical preview of {GitHub} Copilot, a new {AI} pair programmer that helps you write better code.},
	titleaddon = {The {GitHub} Blog},
	urldate = {2022-06-27},
	date = {2021-06-29},
	langid = {american},
	file = {OpenAI представила усовершенствованную версию нейросети Codex, которая переводит английские фразы в программный код / Хабр:/Users/germanarutunov/Zotero/storage/2BB659JQ/572276.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/CBRCM3L6/2021-06-29-introducing-github-copilot-ai-pair-programmer.html:text/html},
}

@online{noauthor_deepcode_2019-1,
	title = {{DeepCode} learns from {GitHub} project data to give developers {AI}-powered code reviews},
	url = {https://venturebeat.com/2019/08/06/deepcode-learns-from-github-project-data-to-give-developers-ai-powered-code-reviews/},
	abstract = {{DeepCode} is an {AI}-powered code review system for developers, with machine learning systems trained on billions of lines of code from open source projects.},
	titleaddon = {{VentureBeat}},
	urldate = {2022-06-27},
	date = {2019-08-06},
	langid = {american},
	file = {DeepCode — система анализа кода на базе глубинного обучения / Хабр:/Users/germanarutunov/Zotero/storage/ZSH7WLXE/462711.html:text/html;Snapshot:/Users/germanarutunov/Zotero/storage/SVA455FE/deepcode-learns-from-github-project-data-to-give-developers-ai-powered-code-reviews.html:text/html},
}

@misc{kim_pure_2022,
	title = {Pure Transformers are Powerful Graph Learners},
	url = {http://arxiv.org/abs/2207.02505},
	doi = {10.48550/arXiv.2207.02505},
	abstract = {We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-{IGN}) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks ({GNN}). When trained on a large-scale graph dataset ({PCQM}4Mv2), our method coined Tokenized Graph Transformer ({TokenGT}) achieves significantly better results compared to {GNN} baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.},
	number = {{arXiv}:2207.02505},
	publisher = {{arXiv}},
	author = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
	urldate = {2022-09-04},
	date = {2022-07-06},
	eprinttype = {arxiv},
	eprint = {2207.02505 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/XG8D42VK/Kim et al. - 2022 - Pure Transformers are Powerful Graph Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/LHSV6M2I/2207.html:text/html},
}

@article{__2022,
	title = {Обзор нейросетевых методов анализа и генерации кода},
	volume = {28},
	issn = {1684-6400},
	url = {https://www.elibrary.ru/item.asp?id=49239218},
	doi = {10.17587/it.28.378-391},
	abstract = {В условиях пандемии как никогда стала актуальной проблема нехватки кадров в сфере информационных технологий. По оценкам аналитиков в 2021 году Россия не досчиталась от 500 тыс. до 1 млн {IT}-специалистов. Образование и вывод на рынок такого большого числа специалистов может занять годы. Очень остро стоит вопрос оптимизации процесса создания {IT}-решений, в том числе путем разработки автоматизированных способов решения рутинных задач. В данной статье представлен обзор методов применения нейросетевых технологий в области анализа и генерации программного кода.},
	number = {7},
	journaltitle = {Информационные Технологии},
	author = {Авдошин, С. М. and Арутюнов, Г. А.},
	urldate = {2022-10-10},
	date = {2022},
	langid = {russian},
	keywords = {Анализ Программного Кода, Графовые Нейронные Сети, Искусственный Интеллект, Машинное Программирование, Методы Генерации Кода, Нейросетевые Технологии, Обучение С Подкреплением},
}

@misc{ying_transformers_2021,
	title = {Do Transformers Really Perform Bad for Graph Representation?},
	url = {http://arxiv.org/abs/2106.05234},
	doi = {10.48550/arXiv.2106.05234},
	abstract = {The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream {GNN} variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent {OGB} Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular {GNN} variants could be covered as the special cases of Graphormer.},
	number = {{arXiv}:2106.05234},
	publisher = {{arXiv}},
	author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
	urldate = {2022-10-10},
	date = {2021-11-23},
	eprinttype = {arxiv},
	eprint = {2106.05234 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/Z3Q453UY/Ying et al. - 2021 - Do Transformers Really Perform Bad for Graph Repre.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/LDIFJSR8/2106.html:text/html},
}

@misc{tipirneni_structcoder_2022,
	title = {{StructCoder}: Structure-Aware Transformer for Code Generation},
	url = {http://arxiv.org/abs/2206.05239},
	doi = {10.48550/arXiv.2206.05239},
	shorttitle = {{StructCoder}},
	abstract = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This work addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies that are primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also ensure that our decoder preserves the syntax and data flow of the target code by introducing two auxiliary tasks: {AST} (Abstract Syntax Tree) paths prediction and data flow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder to enhance the quality of generated code by modeling target syntax and data flow. The proposed {StructCoder} model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the {CodeXGLUE} benchmark.},
	number = {{arXiv}:2206.05239},
	publisher = {{arXiv}},
	author = {Tipirneni, Sindhu and Zhu, Ming and Reddy, Chandan K.},
	urldate = {2022-10-11},
	date = {2022-06-10},
	eprinttype = {arxiv},
	eprint = {2206.05239 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/PHUSVMGQ/Tipirneni et al. - 2022 - StructCoder Structure-Aware Transformer for Code .pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/WN6L9YX3/2206.html:text/html},
}

@misc{lakshminarayanan_simple_2017,
	title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
	url = {http://arxiv.org/abs/1612.01474},
	doi = {10.48550/arXiv.1612.01474},
	abstract = {Deep neural networks ({NNs}) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in {NNs} is a challenging and yet unsolved problem. Bayesian {NNs}, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) {NNs}. We propose an alternative to Bayesian {NNs} that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian {NNs}. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on {ImageNet}.},
	number = {{arXiv}:1612.01474},
	publisher = {{arXiv}},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	urldate = {2022-10-15},
	date = {2017-11-03},
	eprinttype = {arxiv},
	eprint = {1612.01474 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/UMM3EXUG/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/4V8UUGAM/1612.html:text/html},
}

@misc{stadler_graph_2021,
	title = {Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification},
	url = {http://arxiv.org/abs/2110.14012},
	doi = {10.48550/arXiv.2110.14012},
	shorttitle = {Graph Posterior Network},
	abstract = {The interdependence between nodes in graphs is key to improve class predictions on nodes and utilized in approaches like Label Propagation ({LP}) or in Graph Neural Networks ({GNN}). Nonetheless, uncertainty estimation for non-independent node-level predictions is under-explored. In this work, we explore uncertainty quantification for node classification in three ways: (1) We derive three axioms explicitly characterizing the expected predictive uncertainty behavior in homophilic attributed graphs. (2) We propose a new model Graph Posterior Network ({GPN}) which explicitly performs Bayesian posterior updates for predictions on interdependent nodes. {GPN} provably obeys the proposed axioms. (3) We extensively evaluate {GPN} and a strong set of baselines on semi-supervised node classification including detection of anomalous features, and detection of left-out classes. {GPN} outperforms existing approaches for uncertainty estimation in the experiments.},
	number = {{arXiv}:2110.14012},
	publisher = {{arXiv}},
	author = {Stadler, Maximilian and Charpentier, Bertrand and Geisler, Simon and Zügner, Daniel and Günnemann, Stephan},
	urldate = {2022-10-15},
	date = {2021-10-26},
	eprinttype = {arxiv},
	eprint = {2110.14012 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/7SKAGAGG/Stadler et al. - 2021 - Graph Posterior Network Bayesian Predictive Uncer.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/KH6FZQ3T/2110.html:text/html},
}

@misc{hamilton_inductive_2018,
	title = {Inductive Representation Learning on Large Graphs},
	url = {http://arxiv.org/abs/1706.02216},
	doi = {10.48550/arXiv.1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present {GraphSAGE}, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	number = {{arXiv}:1706.02216},
	publisher = {{arXiv}},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2022-10-15},
	date = {2018-09-10},
	eprinttype = {arxiv},
	eprint = {1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/F9UBH38D/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/Q6RIZEMT/1706.html:text/html},
}

@misc{hu_graph-mlp_2021,
	title = {Graph-{MLP}: Node Classification without Message Passing in Graph},
	url = {http://arxiv.org/abs/2106.04051},
	doi = {10.48550/arXiv.2106.04051},
	shorttitle = {Graph-{MLP}},
	abstract = {Graph Neural Network ({GNN}) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based {GNNs} are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-{MLP} with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-{MLP} only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive ({NContrast}) loss to bridge the gap between {GNNs} and {MLPs} by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.},
	number = {{arXiv}:2106.04051},
	publisher = {{arXiv}},
	author = {Hu, Yang and You, Haoxuan and Wang, Zhecan and Wang, Zhicheng and Zhou, Erjin and Gao, Yue},
	urldate = {2022-10-16},
	date = {2021-06-07},
	eprinttype = {arxiv},
	eprint = {2106.04051 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/B23ZEHQL/Hu et al. - 2021 - Graph-MLP Node Classification without Message Pas.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/ZU7YUF7Q/2106.html:text/html},
}

@article{kleinberg_hubs_1999,
	title = {Hubs, authorities, and communities},
	volume = {31},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/345966.345982},
	doi = {10.1145/345966.345982},
	pages = {5--es},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Kleinberg, Jon M.},
	urldate = {2022-10-16},
	date = {1999-12-01},
	keywords = {graph algorithms, hypertext structure, link analysis, World Wide Web},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/N4XJPPV2/Kleinberg - 1999 - Hubs, authorities, and communities.pdf:application/pdf},
}

@article{lempel_salsa_2001,
	title = {{SALSA}: the stochastic approach for link-structure analysis},
	volume = {19},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/382979.383041},
	doi = {10.1145/382979.383041},
	shorttitle = {{SALSA}},
	abstract = {Today, when searching for information on the {WWW}, one usually performs a query through a term-based search engine. These engines return, as the query's result, a list of Web pages whose contents matches the query. For broad-topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. However, much information is contained in the link-structure of the {WWW}. Information such as which pages are linked to others can be used to augment search algorithms. In this context, Jon Kleinberg introduced the notion of two distinct types of Web pages: hubs and authorities. Kleinberg argued that hubs and authorities exhibit a mutually reinforcing relationship: a good hub will point to many authorities, and a good authority will be pointed at by many hubs. In light of this, he dervised an algoirthm aimed at finding authoritative pages. We present {SALSA}, a new stochastic approach for link-structure analysis, which examines random walks on graphs derived from the link-structure. We show that both {SALSA} and Kleinberg's Mutual Reinforcement approach employ the same metaalgorithm. We then prove that {SALSA} is quivalent to a weighted in degree analysis of the link-sturcutre of {WWW} subgraphs, making it computationally more efficient than the Mutual reinforcement approach. We compare that results of applying {SALSA} to the results derived through Kleinberg's approach. These comparisions reveal a topological Phenomenon called the {TKC} effectwhich, in certain cases, prevents the Mutual reinforcement approach from identifying meaningful authorities.},
	pages = {131--160},
	number = {2},
	journaltitle = {{ACM} Transactions on Information Systems},
	shortjournal = {{ACM} Trans. Inf. Syst.},
	author = {Lempel, R. and Moran, S.},
	urldate = {2022-10-16},
	date = {2001-04-01},
	keywords = {hubs and authorities, Link-structure analysis, random walks, {SALSA}, {TKC} effect},
}

@article{borodin_link_2005,
	title = {Link analysis ranking: algorithms, theory, and experiments},
	volume = {5},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/1052934.1052942},
	doi = {10.1145/1052934.1052942},
	shorttitle = {Link analysis ranking},
	abstract = {The explosive growth and the widespread accessibility of the Web has led to a surge of research activity in the area of information retrieval on the World Wide Web. The seminal papers of Kleinberg [1998, 1999] and Brin and Page [1998] introduced Link Analysis Ranking, where hyperlink structures are used to determine the relative authority of a Web page and produce improved algorithms for the ranking of Web search results. In this article we work within the hubs and authorities framework defined by Kleinberg and we propose new families of algorithms. Two of the algorithms we propose use a Bayesian approach, as opposed to the usual algebraic and graph theoretic approaches. We also introduce a theoretical framework for the study of Link Analysis Ranking algorithms. The framework allows for the definition of specific properties of Link Analysis Ranking algorithms, as well as for comparing different algorithms. We study the properties of the algorithms that we define, and we provide an axiomatic characterization of the {INDEGREE} heuristic which ranks each node according to the number of incoming links. We conclude the article with an extensive experimental evaluation. We study the quality of the algorithms, and we examine how different structures in the graphs affect their performance.},
	pages = {231--297},
	number = {1},
	journaltitle = {{ACM} Transactions on Internet Technology},
	shortjournal = {{ACM} Trans. Internet Technol.},
	author = {Borodin, Allan and Roberts, Gareth O. and Rosenthal, Jeffrey S. and Tsaparas, Panayiotis},
	urldate = {2022-10-16},
	date = {2005-02-01},
	keywords = {link analysis, Bayesian, {HITS}, ranking, Web search},
}

@article{huang_snapshot_2017,
	title = {Snapshot Ensembles: Train 1, get M for free},
	url = {https://arxiv.org/abs/1704.00109v1},
	doi = {10.48550/arXiv.1704.00109},
	shorttitle = {Snapshot Ensembles},
	abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On {CIFAR}-10 and {CIFAR}-100 our {DenseNet} Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
	author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
	urldate = {2022-10-16},
	date = {2017-04-01},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/87RHA22Q/Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/GK6A83HB/1704.html:text/html},
}

@misc{garipov_loss_2018,
	title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of {DNNs}},
	url = {http://arxiv.org/abs/1802.10026},
	doi = {10.48550/arXiv.1802.10026},
	abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling ({FGE}). Using {FGE} we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on {CIFAR}-10, {CIFAR}-100, and {ImageNet}.},
	number = {{arXiv}:1802.10026},
	publisher = {{arXiv}},
	author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
	urldate = {2022-10-16},
	date = {2018-10-30},
	eprinttype = {arxiv},
	eprint = {1802.10026 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/ETLBBD58/Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembl.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/WU7FZSFM/1802.html:text/html},
}

@misc{simpson_learning_2022,
	title = {Learning Structured Gaussians to Approximate Deep Ensembles},
	url = {http://arxiv.org/abs/2203.15485},
	doi = {10.48550/arXiv.2203.15485},
	abstract = {This paper proposes using a sparse-structured multivariate Gaussian to provide a closed-form approximator for the output of probabilistic ensemble models used for dense image prediction tasks. This is achieved through a convolutional neural network that predicts the mean and covariance of the distribution, where the inverse covariance is parameterised by a sparsely structured Cholesky matrix. Similarly to distillation approaches, our single network is trained to maximise the probability of samples from pre-trained probabilistic models, in this work we use a fixed ensemble of networks. Once trained, our compact representation can be used to efficiently draw spatially correlated samples from the approximated output distribution. Importantly, this approach captures the uncertainty and structured correlations in the predictions explicitly in a formal distribution, rather than implicitly through sampling alone. This allows direct introspection of the model, enabling visualisation of the learned structure. Moreover, this formulation provides two further benefits: estimation of a sample probability, and the introduction of arbitrary spatial conditioning at test time. We demonstrate the merits of our approach on monocular depth estimation and show that the advantages of our approach are obtained with comparable quantitative performance.},
	number = {{arXiv}:2203.15485},
	publisher = {{arXiv}},
	author = {Simpson, Ivor J. A. and Vicente, Sara and Campbell, Neill D. F.},
	urldate = {2022-10-16},
	date = {2022-03-29},
	eprinttype = {arxiv},
	eprint = {2203.15485 [cs, eess, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/MZPSRC2S/Simpson et al. - 2022 - Learning Structured Gaussians to Approximate Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/AJTH3SPZ/2203.html:text/html},
}

@misc{malinin_predictive_2018,
	title = {Predictive Uncertainty Estimation via Prior Networks},
	url = {http://arxiv.org/abs/1802.10501},
	doi = {10.48550/arXiv.1802.10501},
	abstract = {Estimating how uncertain an {AI} system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks ({PNs}) which explicitly models distributional uncertainty. {PNs} do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates {PNs} on the tasks of identifying out-of-distribution ({OOD}) samples and detecting misclassification on the {MNIST} dataset, where they are found to outperform previous methods. Experiments on synthetic and {MNIST} and {CIFAR}-10 data show that unlike previous non-Bayesian methods {PNs} are able to distinguish between data and distributional uncertainty.},
	number = {{arXiv}:1802.10501},
	publisher = {{arXiv}},
	author = {Malinin, Andrey and Gales, Mark},
	urldate = {2022-10-16},
	date = {2018-11-29},
	eprinttype = {arxiv},
	eprint = {1802.10501 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/KTIFDFHL/Malinin and Gales - 2018 - Predictive Uncertainty Estimation via Prior Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/UUTD9DMF/1802.html:text/html},
}

@misc{charpentier_natural_2022,
	title = {Natural Posterior Network: Deep Bayesian Uncertainty for Exponential Family Distributions},
	url = {http://arxiv.org/abs/2105.04471},
	doi = {10.48550/arXiv.2105.04471},
	shorttitle = {Natural Posterior Network},
	abstract = {Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network ({NatPN}) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, {NatPN} finds application for both classification and general regression settings. Unlike many previous approaches, {NatPN} does not require out-of-distribution ({OOD}) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, {NatPN} uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, {NatPN} assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and {OOD} detection show that {NatPN} delivers highly competitive performance for classification, regression and count prediction tasks.},
	number = {{arXiv}:2105.04471},
	publisher = {{arXiv}},
	author = {Charpentier, Bertrand and Borchert, Oliver and Zügner, Daniel and Geisler, Simon and Günnemann, Stephan},
	urldate = {2022-10-16},
	date = {2022-03-16},
	eprinttype = {arxiv},
	eprint = {2105.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/9ZDY3NHZ/Charpentier et al. - 2022 - Natural Posterior Network Deep Bayesian Uncertain.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/GLY3V35Q/2105.html:text/html},
}

@online{page_pagerank_1999,
	title = {The {PageRank} Citation Ranking: Bringing Order to the Web.},
	url = {http://ilpubs.stanford.edu:8090/422/},
	shorttitle = {The {PageRank} Citation Ranking},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes {PageRank}, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare {PageRank} to an idealized random Web surfer. We show how to efficiently compute {PageRank} for large numbers of pages. And, we show how to apply {PageRank} to search and to user navigation.},
	type = {Techreport},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	urldate = {2022-10-19},
	date = {1999-11-11},
	note = {Publisher: Stanford {InfoLab}},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/TH5I56IL/Page et al. - 1999 - The PageRank Citation Ranking Bringing Order to t.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/CQEJ6QR6/422.html:text/html},
}

@misc{malinin_shifts_2022,
	title = {Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks},
	url = {http://arxiv.org/abs/2107.07455},
	doi = {10.48550/arXiv.2107.07455},
	shorttitle = {Shifts},
	abstract = {There has been significant research done on developing methods for improving robustness to distributional shift and uncertainty estimation. In contrast, only limited work has examined developing standard datasets and benchmarks for assessing these approaches. Additionally, most work on uncertainty estimation and robustness has developed new techniques based on small-scale regression or image classification tasks. However, many tasks of practical interest have different modalities, such as tabular data, audio, text, or sensor data, which offer significant challenges involving regression and discrete or continuous structured prediction. Thus, given the current state of the field, a standardized large-scale dataset of tasks across a range of modalities affected by distributional shifts is necessary. This will enable researchers to meaningfully evaluate the plethora of recently developed uncertainty quantification methods, as well as assessment criteria and state-of-the-art baselines. In this work, we propose the Shifts Dataset for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car ({SDC}) vehicle motion prediction. All of these data modalities and tasks are affected by real, "in-the-wild" distributional shifts and pose interesting challenges with respect to uncertainty estimation. In this work we provide a description of the dataset and baseline results for all tasks.},
	number = {{arXiv}:2107.07455},
	publisher = {{arXiv}},
	author = {Malinin, Andrey and Band, Neil and Ganshin and Alexander and Chesnokov, German and Gal, Yarin and Gales, Mark J. F. and Noskov, Alexey and Ploskonosov, Andrey and Prokhorenkova, Liudmila and Provilkov, Ivan and Raina, Vatsal and Raina, Vyas and Roginskiy and Denis and Shmatova, Mariya and Tigas, Panos and Yangel, Boris},
	urldate = {2022-10-19},
	date = {2022-02-11},
	eprinttype = {arxiv},
	eprint = {2107.07455 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/JHH8ERU3/Malinin et al. - 2022 - Shifts A Dataset of Real Distributional Shift Acr.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/8VKBGACW/2107.html:text/html},
}

@misc{malinin_shifts_2022-1,
	title = {Shifts 2.0: Extending The Dataset of Real Distributional Shifts},
	url = {http://arxiv.org/abs/2206.15407},
	doi = {10.48550/arXiv.2206.15407},
	shorttitle = {Shifts 2.0},
	abstract = {Distributional shift, or the mismatch between training and deployment data, is a significant obstacle to the usage of machine learning in high-stakes industrial applications, such as autonomous driving and medicine. This creates a need to be able to assess how robustly {ML} models generalize as well as the quality of their uncertainty estimates. Standard {ML} baseline datasets do not allow these properties to be assessed, as the training, validation and test data are often identically distributed. Recently, a range of dedicated benchmarks have appeared, featuring both distributionally matched and shifted data. Among these benchmarks, the Shifts dataset stands out in terms of the diversity of tasks as well as the data modalities it features. While most of the benchmarks are heavily dominated by 2D image classification tasks, Shifts contains tabular weather forecasting, machine translation, and vehicle motion prediction tasks. This enables the robustness properties of models to be assessed on a diverse set of industrial-scale tasks and either universal or directly applicable task-specific conclusions to be reached. In this paper, we extend the Shifts Dataset with two datasets sourced from industrial, high-risk applications of high societal importance. Specifically, we consider the tasks of segmentation of white matter Multiple Sclerosis lesions in 3D magnetic resonance brain images and the estimation of power consumption in marine cargo vessels. Both tasks feature ubiquitous distributional shifts and a strict safety requirement due to the high cost of errors. These new datasets will allow researchers to further explore robust generalization and uncertainty estimation in new situations. In this work, we provide a description of the dataset and baseline results for both tasks.},
	number = {{arXiv}:2206.15407},
	publisher = {{arXiv}},
	author = {Malinin, Andrey and Athanasopoulos, Andreas and Barakovic, Muhamed and Cuadra, Meritxell Bach and Gales, Mark J. F. and Granziera, Cristina and Graziani, Mara and Kartashev, Nikolay and Kyriakopoulos, Konstantinos and Lu, Po-Jui and Molchanova, Nataliia and Nikitakis, Antonis and Raina, Vatsal and La Rosa, Francesco and Sivena, Eli and Tsarsitalidis, Vasileios and Tsompopoulou, Efi and Volf, Elena},
	urldate = {2022-10-19},
	date = {2022-09-15},
	eprinttype = {arxiv},
	eprint = {2206.15407 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/E2RF3ZK9/Malinin et al. - 2022 - Shifts 2.0 Extending The Dataset of Real Distribu.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/GG3SDEJ5/2206.html:text/html},
}

@inproceedings{gal_uncertainty_2016,
	title = {Uncertainty in Deep Learning},
	author = {Gal, Yarin},
	date = {2016},
}

@thesis{malinin_uncertainty_2019,
	title = {Uncertainty Estimation in Deep Learning with application to Spoken Language Assessment},
	rights = {All rights reserved},
	url = {https://www.repository.cam.ac.uk/handle/1810/298857},
	abstract = {Since convolutional neural networks ({CNNs}) achieved top performance on the {ImageNet} task in 2012, deep learning has become the preferred approach to addressing computer vision, natural language processing, speech recognition and bio-informatics tasks. However, despite impressive performance, neural networks tend to make over-confident predictions. Thus, it is necessary to investigate robust, interpretable and tractable estimates of uncertainty in a model's predictions in order to construct safer Machine Learning systems. This is crucial to applications where the cost of an error is high, such as in autonomous vehicle control, high-stakes automatic proficiency assessment and in the medical, financial and legal fields.  
 
In the first part of this thesis uncertainty estimation via ensemble and single-model approaches is discussed in detail and a new class of models for uncertainty estimation, called \${\textbackslash}textit\{Prior Networks\}\$, is proposed. Prior Networks are able to \${\textbackslash}textit\{emulate\}\$ an ensemble of models using a single deterministic neural network, which allows sources of uncertainty to be determined within the same probabilistic framework as in ensemble-based approaches, but with the computational simplicity and ease of training of single-model approaches. Thus, Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty. In this thesis Prior Networks are evaluated on a range classification datasets, where they are shown to outperform baseline approaches, such as Monte-Carlo dropout, on the task of detecting out-of-distribution inputs. 
 
In the second part of this thesis deep learning and uncertainty estimation approaches are applied to the area of automatic assessment of non-native spoken language proficiency. Specifically deep-learning based graders and spoken response relevance assessment systems are constructed using data from the {BULATS} and {LinguaSkill} exams, provided by Cambridge English Language Assessment. Baseline approaches for uncertainty estimation discussed and evaluated in the first half of the thesis are then applied to these models and assessed on the task of rejecting predictions to be graded by human examiners and detecting misclassifications.},
	institution = {University of Cambridge},
	type = {Thesis},
	author = {Malinin, Andrey},
	urldate = {2022-10-19},
	date = {2019-10-26},
	langid = {english},
	doi = {10.17863/CAM.45912},
	note = {Accepted: 2019-11-13T09:11:29Z},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/V74QB7HP/Malinin - 2019 - Uncertainty Estimation in Deep Learning with appli.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/I4C8N8M4/298857.html:text/html},
}

@article{mccallum_automating_2000,
	title = {Automating the Construction of Internet Portals with Machine Learning},
	volume = {3},
	issn = {1573-7659},
	url = {https://doi.org/10.1023/A:1009953814988},
	doi = {10.1023/A:1009953814988},
	abstract = {Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains.},
	pages = {127--163},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Information Retrieval},
	author = {{McCallum}, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
	urldate = {2022-10-20},
	date = {2000-07-01},
	langid = {english},
	keywords = {crawling, expectation-maximization, hidden Markov models, information extraction, naive Bayes, reinforcement learning, spidering, text classification, unlabeled data},
}

@inproceedings{giles_citeseer_1998,
	location = {New York, {NY}, {USA}},
	title = {{CiteSeer}: an automatic citation indexing system},
	isbn = {978-0-89791-965-4},
	url = {https://doi.org/10.1145/276675.276685},
	doi = {10.1145/276675.276685},
	series = {{DL} '98},
	shorttitle = {{CiteSeer}},
	pages = {89--98},
	booktitle = {Proceedings of the third {ACM} conference on Digital libraries},
	publisher = {Association for Computing Machinery},
	author = {Giles, C. Lee and Bollacker, Kurt D. and Lawrence, Steve},
	urldate = {2022-10-20},
	date = {1998-05-11},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/FESZH2Y3/Giles et al. - 1998 - CiteSeer an automatic citation indexing system.pdf:application/pdf},
}

@incollection{getoor_link-based_2005,
	location = {London},
	title = {Link-based Classification},
	isbn = {978-1-84628-284-3},
	url = {https://doi.org/10.1007/1-84628-284-5_7},
	series = {Advanced Information and Knowledge Processing},
	abstract = {A key challenge for machine learning is the problem of mining richly structured data sets, where the objects are linked in some way due to either an explicit or implicit relationship that exists between the objects. Links among the objects demonstrate certain patterns, which can be helpful for many machine learning tasks and are usually hard to capture with traditional statistical models. Recently there has been a surge of interest in this area, fuelled largely by interest in web and hypertext mining, but also by interest in mining social networks, bibliographic citation data, epidemiological data and other domains best described using a linked or graph structure. In this chapter we propose a framework for modeling link distributions, a link-based model that supports discriminative models describing both the link distributions and the attributes of linked objects. We use a structured logistic regression model, capturing both content and links. We systematically evaluate several variants of our link-based model on a range of data sets including both web and citation collections. In all cases, the use of the link distribution improves classification performance.},
	pages = {189--207},
	booktitle = {Advanced Methods for Knowledge Discovery from Complex Data},
	publisher = {Springer},
	author = {Getoor, Lise},
	editor = {Bandyopadhyay, Sanghamitra and Maulik, Ujjwal and Holder, Lawrence B. and Cook, Diane J.},
	urldate = {2022-10-20},
	date = {2005},
	langid = {english},
	doi = {10.1007/1-84628-284-5_7},
	keywords = {Inductive Logic Programming, Link Feature, Link Mining, Remove Stop Word, Unlabeled Data},
	file = {Submitted Version:/Users/germanarutunov/Zotero/storage/SWG6H6Z6/Getoor - 2005 - Link-based Classification.pdf:application/pdf},
}

@article{sen_collective_2008,
	title = {Collective Classification in Network Data},
	volume = {29},
	rights = {Copyright (c) 0},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2157},
	doi = {10.1609/aimag.v29i3.2157},
	abstract = {Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.},
	pages = {93--93},
	number = {3},
	journaltitle = {{AI} Magazine},
	author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
	urldate = {2022-10-20},
	date = {2008-09-06},
	langid = {english},
	note = {Number: 3},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/J5R79HXY/Sen et al. - 2008 - Collective Classification in Network Data.pdf:application/pdf},
}

@inproceedings{namata_query-driven_2012,
	title = {Query-driven Active Surveying for Collective Classification},
	author = {Namata, Galileo and London, Ben and Getoor, Lise and Huang, Bert},
	date = {2012},
}

@misc{shchur_pitfalls_2019,
	title = {Pitfalls of Graph Neural Network Evaluation},
	url = {http://arxiv.org/abs/1811.05868},
	doi = {10.48550/arXiv.1811.05868},
	abstract = {Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks ({GNNs}) have achieved unparalleled results on this task. Due to their massive success, {GNNs} have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for {GNN} models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent {GNN} models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler {GNN} architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.},
	number = {{arXiv}:1811.05868},
	publisher = {{arXiv}},
	author = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and Günnemann, Stephan},
	urldate = {2022-10-20},
	date = {2019-06-18},
	eprinttype = {arxiv},
	eprint = {1811.05868 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/JCT33HBC/Shchur et al. - 2019 - Pitfalls of Graph Neural Network Evaluation.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/7MLTM9YR/1811.html:text/html},
}

@misc{mcauley_image-based_2015,
	title = {Image-based Recommendations on Styles and Substitutes},
	url = {http://arxiv.org/abs/1506.04757},
	doi = {10.48550/arXiv.1506.04757},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	number = {{arXiv}:1506.04757},
	publisher = {{arXiv}},
	author = {{McAuley}, Julian and Targett, Christopher and Shi, Qinfeng and Hengel, Anton van den},
	urldate = {2022-10-20},
	date = {2015-06-15},
	eprinttype = {arxiv},
	eprint = {1506.04757 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/2EDYFIPA/McAuley et al. - 2015 - Image-based Recommendations on Styles and Substitu.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/3TZ59GLF/1506.html:text/html},
}

@inproceedings{grishanov_multiobjective_2022,
	location = {New York, {NY}, {USA}},
	title = {Multiobjective Evaluation of Reinforcement Learning Based Recommender Systems},
	isbn = {978-1-4503-9278-5},
	url = {https://doi.org/10.1145/3523227.3551485},
	doi = {10.1145/3523227.3551485},
	series = {{RecSys} '22},
	abstract = {Movielens dataset has become a default choice for recommender systems evaluation. In this paper we analyze the best strategies of a Reinforcement Learning agent on Movielens (1M) dataset studying the balance between precision and diversity of recommendations. We found that trivial strategies are able to maximize ranking quality criteria, but useless for users of the recommendation system due to the lack of diversity in final predictions. Our proposed method stimulates the agent to explore the environment using the stochasticity of Ornstein-Uhlenbeck processes. Experiments show that optimization of the Ornstein-Uhlenbeck process drift coefficient improves the diversity of recommendations while maintaining high {nDCG} and {HR} criteria. To the best of our knowledge, the analysis of agent strategies in recommendation environments has not been studied excessively in previous works.},
	pages = {622--627},
	booktitle = {Proceedings of the 16th {ACM} Conference on Recommender Systems},
	publisher = {Association for Computing Machinery},
	author = {Grishanov, Alexey and Ianina, Anastasia and Vorontsov, Konstantin},
	urldate = {2022-10-21},
	date = {2022-09-18},
	keywords = {Deep Deterministic Policy Gradient ({DDPG}), Deep Reinforcement Learning, noise injection, Ornstein-Uhlenbeck processes, Recommendation Systems},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/9YQW5LN3/Grishanov et al. - 2022 - Multiobjective Evaluation of Reinforcement Learnin.pdf:application/pdf},
}

@inproceedings{chen_values_2021,
	location = {New York, {NY}, {USA}},
	title = {Values of User Exploration in Recommender Systems},
	isbn = {978-1-4503-8458-2},
	url = {https://doi.org/10.1145/3460231.3474236},
	doi = {10.1145/3460231.3474236},
	series = {{RecSys} '21},
	abstract = {Reinforcement Learning ({RL}) has been sought after to bring next-generation recommender systems to further improve user experience on recommendation platforms. While the exploration-exploitation tradeoff is the foundation of {RL} research, the value of exploration in ({RL}-based) recommender systems is less well understood. Exploration, commonly seen as a tool to reduce model uncertainty in regions of sparse user interaction/feedback, is believed to cost user experience in the short term, while the indirect benefit of better model quality arrives at a later time. We focus on another aspect of exploration, which we refer to as user exploration to help discover new user interests, and argue it can improve user experience even in the more imminent term. We examine the role of user exploration in changing different facets of recommendation quality that more directly impact user experience. To do so, we introduce a series of methods inspired by exploration research in {RL} to increase user exploration in an {RL}-based recommender system, and study their effect on the end recommendation quality, more specifically, on accuracy, diversity, novelty and serendipity. We propose a set of metrics to measure ({RL} based) recommender systems in these four aspects and evaluate the impact of exploration-induced methods against these metrics. In addition to the offline measurements, we conduct live experiments on an industrial recommendation platform serving billions of users to showcase the benefit of user exploration. Moreover, we use conversion of casual users to core users as an indicator of the holistic long-term user experience and study the values of user exploration in helping platforms convert users. Through offline analyses and live experiments, we study the correlation between these four facets of recommendation quality and long term user experience, and connect serendipity to improved long term user experience.},
	pages = {85--95},
	booktitle = {Proceedings of the 15th {ACM} Conference on Recommender Systems},
	publisher = {Association for Computing Machinery},
	author = {Chen, Minmin and Wang, Yuyan and Xu, Can and Le, Ya and Sharma, Mohit and Richardson, Lee and Wu, Su-Lin and Chi, Ed},
	urldate = {2022-10-21},
	date = {2021-09-13},
	keywords = {reinforcement learning, exploration, recommender systems, serendipity},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/J262Q3RJ/Chen et al. - 2021 - Values of User Exploration in Recommender Systems.pdf:application/pdf},
}

@inproceedings{chen_exploration_2021,
	location = {New York, {NY}, {USA}},
	title = {Exploration in Recommender Systems},
	isbn = {978-1-4503-8458-2},
	url = {https://doi.org/10.1145/3460231.3474601},
	doi = {10.1145/3460231.3474601},
	series = {{RecSys} '21},
	abstract = {In the era of increasing choices, recommender systems are becoming indispensable in helping users navigate the million or billion pieces of content on recommendation platforms. As the focus of these systems shifts from attracting short-term user attention toward optimizing long term user experience on these platforms, reinforcement learning (and bandits) have emerged as appealing techniques to power these systems [5, 9, 26, 27]. The exploration-exploitation tradeoff, being the foundation of bandits and {RL} research, has been extensively studied [1, 2, 4, 6, 8, 10, 11, 18, 20, 21, 22, 23]. An agent is incentivized to exploit to maximize its return, i.e., by repeating actions taken in the past that produced high rewards. On the other hand, the agent needs to explore previously unseen actions in order to discover potentially better ones. Exploration has been shown to be extremely useful in solving tasks of long horizons or sparse reward in many {RL} applications [2, 14, 15, 16, 19]. While effective exploration is believed to positively influence the user experience on the platform, the exact value of exploration in recommender systems has not been well established. In this talk, we examine the roles of exploration in recommender systems in three facets: 1) system exploration to reduce system uncertainty in regions with sparse feedback; 2) user exploration to introduce users to new interests/tastes; and 3) online exploration to take into account real-time user feedback. We showcase how each aspect of exploration contributes to the long term user experience through offline and live experiments on industrial recommendation platforms. We hope this talk can inspire more follow up work in understanding and improving exploration in recommender systems.},
	pages = {551--553},
	booktitle = {Proceedings of the 15th {ACM} Conference on Recommender Systems},
	publisher = {Association for Computing Machinery},
	author = {Chen, Minmin},
	urldate = {2022-10-21},
	date = {2021-09-13},
	keywords = {reinforcement learning, exploration, recommender systems, serendipity, bandits, online learning, uncertainty},
}

@article{yang_exploration_2021,
	title = {Exploration in Deep Reinforcement Learning: A Comprehensive Survey},
	url = {https://arxiv.org/abs/2109.06668v4},
	doi = {10.48550/arXiv.2109.06668},
	shorttitle = {Exploration in Deep Reinforcement Learning},
	abstract = {Deep Reinforcement Learning ({DRL}) and Deep Multi-agent Reinforcement Learning ({MARL}) have achieved significant successes across a wide range of domains, including game {AI}, autonomous vehicles, robotics, and so on. However, {DRL} and deep {MARL} agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning towards the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners. In this paper, we conduct a comprehensive survey on existing exploration methods for both single-agent and multi-agent {RL}. We start the survey by identifying several key challenges to efficient exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for {DRL} on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in {DRL} and deep {MARL} and point out a few future directions.},
	author = {Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Hao, Jianye and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
	urldate = {2022-10-22},
	date = {2021-09-14},
	langid = {english},
	file = {Full Text PDF:/Users/germanarutunov/Zotero/storage/93AVEGVK/Yang et al. - 2021 - Exploration in Deep Reinforcement Learning A Comp.pdf:application/pdf;Snapshot:/Users/germanarutunov/Zotero/storage/2HHA6PUD/2109.html:text/html},
}

@article{uhlenbeck_theory_1930,
	title = {On the Theory of the Brownian Motion},
	volume = {36},
	url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
	doi = {10.1103/PhysRev.36.823},
	pages = {823--841},
	number = {5},
	journaltitle = {Phys. Rev.},
	author = {Uhlenbeck, G. E. and Ornstein, L. S.},
	date = {1930-09},
	note = {Publisher: American Physical Society},
}

@misc{liu_deep_2019,
	title = {Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling},
	url = {http://arxiv.org/abs/1810.12027},
	doi = {10.48550/arXiv.1810.12027},
	abstract = {Recommendation is crucial in both academia and industry, and various techniques are proposed such as content-based collaborative filtering, matrix factorization, logistic regression, factorization machines, neural networks and multi-armed bandits. However, most of the previous studies suffer from two limitations: (1) considering the recommendation as a static procedure and ignoring the dynamic interactive nature between users and the recommender systems, (2) focusing on the immediate feedback of recommended items and neglecting the long-term rewards. To address the two limitations, in this paper we propose a novel recommendation framework based on deep reinforcement learning, called {DRR}. The {DRR} framework treats recommendation as a sequential decision making procedure and adopts an "Actor-Critic" reinforcement learning scheme to model the interactions between the users and recommender systems, which can consider both the dynamic adaptation and long-term rewards. Furthermore, a state representation module is incorporated into {DRR}, which can explicitly capture the interactions between items and users. Three instantiation structures are developed. Extensive experiments on four real-world datasets are conducted under both the offline and online evaluation settings. The experimental results demonstrate the proposed {DRR} method indeed outperforms the state-of-the-art competitors.},
	number = {{arXiv}:1810.12027},
	publisher = {{arXiv}},
	author = {Liu, Feng and Tang, Ruiming and Li, Xutao and Zhang, Weinan and Ye, Yunming and Chen, Haokun and Guo, Huifeng and Zhang, Yuzhou},
	urldate = {2022-10-22},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1810.12027 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/QCIHL29H/Liu et al. - 2019 - Deep Reinforcement Learning based Recommendation w.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/U3J9C5FT/1810.html:text/html},
}

@misc{dulac-arnold_deep_2016,
	title = {Deep Reinforcement Learning in Large Discrete Action Spaces},
	url = {http://arxiv.org/abs/1512.07679},
	doi = {10.48550/arXiv.1512.07679},
	abstract = {Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.},
	number = {{arXiv}:1512.07679},
	publisher = {{arXiv}},
	author = {Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
	urldate = {2022-10-22},
	date = {2016-04-04},
	eprinttype = {arxiv},
	eprint = {1512.07679 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/9DXLQFA7/Dulac-Arnold et al. - 2016 - Deep Reinforcement Learning in Large Discrete Acti.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/6XVU654T/1512.html:text/html},
}

@article{harper_movielens_2015,
	title = {The {MovieLens} Datasets: History and Context},
	volume = {5},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/2827872},
	doi = {10.1145/2827872},
	shorttitle = {The {MovieLens} Datasets},
	abstract = {The {MovieLens} datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the {MovieLens} movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of {MovieLens} and the {MovieLens} datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the {MovieLens} datasets in new research.},
	pages = {19:1--19:19},
	number = {4},
	journaltitle = {{ACM} Transactions on Interactive Intelligent Systems},
	shortjournal = {{ACM} Trans. Interact. Intell. Syst.},
	author = {Harper, F. Maxwell and Konstan, Joseph A.},
	urldate = {2022-10-22},
	date = {2015-12-22},
	keywords = {Datasets, {MovieLens}, ratings, recommendations},
}

@misc{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	doi = {10.48550/arXiv.1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	number = {{arXiv}:1509.02971},
	publisher = {{arXiv}},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	urldate = {2022-10-22},
	date = {2019-07-05},
	eprinttype = {arxiv},
	eprint = {1509.02971 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/BWWYR9EI/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/PYLVB567/1509.html:text/html},
}

@misc{fujimoto_addressing_2018,
	title = {Addressing Function Approximation Error in Actor-Critic Methods},
	url = {http://arxiv.org/abs/1802.09477},
	doi = {10.48550/arXiv.1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of {OpenAI} gym tasks, outperforming the state of the art in every environment tested.},
	number = {{arXiv}:1802.09477},
	publisher = {{arXiv}},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	urldate = {2022-10-23},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1802.09477 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/LJJ6DDLW/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/XZ6GMMMF/1802.html:text/html},
}

@inproceedings{zhan_twin_2021,
	title = {Twin Delayed Multi-Agent Deep Deterministic Policy Gradient},
	doi = {10.1109/PIC53636.2021.9687069},
	abstract = {Recently, reinforcement learning has made remarkable achievements in the fields of natural science, engineering, medicine and operational research. Reinforcement learning addresses sequence problems and considers long-term returns. This long-term view of reinforcement learning is critical to find the optimal solution of many problems. The existing multi- agent reinforcement learning algorithms have the problem of overestimation in estimating the Q value. Unfortunately, there have not been many studies on overestimation of agent reinforcement learning, which will affect the learning efficiency of reinforcement learning. Based on the traditional multi-agent reinforcement learning algorithm, this paper improves the actor network and critic network, optimizes the overestimation of Q value and adopts the update delayed method to make the actor training more stable. In order to test the effectiveness of the algorithm structure, the modified method is compared with the traditional {MADDPG}, {DDPG} and {DQN} methods in the simulation environment.},
	eventtitle = {2021 {IEEE} International Conference on Progress in Informatics and Computing ({PIC})},
	pages = {48--52},
	booktitle = {2021 {IEEE} International Conference on Progress in Informatics and Computing ({PIC})},
	author = {Zhan, Mengying and Chen, Jinchao and Du, Chenglie and Duan, Yuxin},
	date = {2021-12},
	note = {{ISSN}: 2329-6259},
	keywords = {Deep learning, Conferences, Convergence, Delays, Informatics, multi-agent system, neural networks, overestimation, Reinforcement learning, Training},
	file = {IEEE Xplore Abstract Record:/Users/germanarutunov/Zotero/storage/RMYR5TSK/9687069.html:text/html},
}

@misc{joulin_bag_2016,
	title = {Bag of Tricks for Efficient Text Classification},
	url = {http://arxiv.org/abs/1607.01759},
	doi = {10.48550/arXiv.1607.01759},
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier {fastText} is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train {fastText} on more than one billion words in less than ten minutes using a standard multicore{\textasciitilde}{CPU}, and classify half a million sentences among{\textasciitilde}312K classes in less than a minute.},
	number = {{arXiv}:1607.01759},
	publisher = {{arXiv}},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	urldate = {2022-11-04},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1607.01759 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/I7Y78FRP/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/RKNR4P6C/1607.html:text/html},
}

@misc{tang_ast-transformer_2021,
	title = {{AST}-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization},
	url = {http://arxiv.org/abs/2112.01184},
	doi = {10.48550/arXiv.2112.01184},
	shorttitle = {{AST}-Transformer},
	abstract = {Code summarization aims to generate brief natural language descriptions for source code. As source code is highly structured and follows strict programming language grammars, its Abstract Syntax Tree ({AST}) is often leveraged to inform the encoder about the structural information. However, {ASTs} are usually much longer than the source code. Current approaches ignore the size limit and simply feed the whole linearized {AST} into the encoder. To address this problem, we propose {AST}-Transformer to efficiently encode tree-structured {ASTs}. Experiments show that {AST}-Transformer outperforms the state-of-arts by a substantial margin while being able to reduce \$90{\textbackslash}sim95{\textbackslash}\%\$ of the computational complexity in the encoding process.},
	number = {{arXiv}:2112.01184},
	publisher = {{arXiv}},
	author = {Tang, Ze and Li, Chuanyi and Ge, Jidong and Shen, Xiaoyu and Zhu, Zheling and Luo, Bin},
	urldate = {2022-11-04},
	date = {2021-12-02},
	eprinttype = {arxiv},
	eprint = {2112.01184 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/7B8GYSTN/Tang et al. - 2021 - AST-Transformer Encoding Abstract Syntax Trees Ef.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/C267UQXN/2112.html:text/html},
}

@misc{zhao_gap-gen_2022,
	title = {{GAP}-Gen: Guided Automatic Python Code Generation},
	url = {http://arxiv.org/abs/2201.08810},
	doi = {10.48550/arXiv.2201.08810},
	shorttitle = {{GAP}-Gen},
	abstract = {Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose {GAP}-Gen, an automatic code generation method guided by Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree ({AST}) reducing the size and high complexity of Abstract Syntax Tree but maintaining the crucial syn-tactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently throughout the code. In our work, rather than pre-training, we focus on modifying the fine-tuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. {GAP}-Gen fine-tunes the transformer-based language models T5 and {CodeT}5 using the Code-to-Docstring datasets {CodeSearchNet}, {CodeSearchNet} {AdvTest}, and Code-Docstring-Corpus from {EdinburghNLP}. Our experiments show that {GAP}-Gen achieves better results on automatic Python code generation task than previous works},
	number = {{arXiv}:2201.08810},
	publisher = {{arXiv}},
	author = {Zhao, Junchen and Song, Yurun and Wang, Junlin and Harris, Ian G.},
	urldate = {2022-11-04},
	date = {2022-01-19},
	eprinttype = {arxiv},
	eprint = {2201.08810 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/M4D2VABK/Zhao et al. - 2022 - GAP-Gen Guided Automatic Python Code Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/VYA62U5S/2201.html:text/html},
}

@inproceedings{miceli_barone_parallel_2017,
	location = {Taipei, Taiwan},
	title = {A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation},
	url = {https://aclanthology.org/I17-2053},
	abstract = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings (“docstrings”) generated by scraping open source repositories on {GitHub}. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data. We release our datasets and processing scripts in order to stimulate research in these areas.},
	pages = {314--319},
	booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Miceli Barone, Antonio Valerio and Sennrich, Rico},
	date = {2017-11},
}

@online{noauthor_edinburghnlpcode-docstring-corpus_nodate,
	title = {{EdinburghNLP}/code-docstring-corpus: Preprocessed Python functions and docstrings for automated code documentation (code2doc) and automated code generation (doc2code) tasks.},
	url = {https://github.com/EdinburghNLP/code-docstring-corpus},
	urldate = {2022-11-04},
	file = {EdinburghNLP/code-docstring-corpus\: Preprocessed Python functions and docstrings for automated code documentation (code2doc) and automated code generation (doc2code) tasks.:/Users/germanarutunov/Zotero/storage/Z2TKHB62/code-docstring-corpus.html:text/html},
}

@misc{sun_treegen_2019,
	title = {{TreeGen}: A Tree-Based Transformer Architecture for Code Generation},
	url = {http://arxiv.org/abs/1911.09983},
	doi = {10.48550/arXiv.1911.09983},
	shorttitle = {{TreeGen}},
	abstract = {A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, {TreeGen}, for code generation. {TreeGen} uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel {AST} reader (encoder) to incorporate grammar rules and {AST} structures into the network. We evaluated {TreeGen} on a Python benchmark, {HearthStone}, and two semantic parsing benchmarks, {ATIS} and {GEO}. {TreeGen} outperformed the previous state-of-the-art approach by 4.5 percentage points on {HearthStone}, and achieved the best accuracy among neural network-based approaches on {ATIS} (89.1\%) and {GEO} (89.6\%). We also conducted an ablation test to better understand each component of our model.},
	number = {{arXiv}:1911.09983},
	publisher = {{arXiv}},
	author = {Sun, Zeyu and Zhu, Qihao and Xiong, Yingfei and Sun, Yican and Mou, Lili and Zhang, Lu},
	urldate = {2022-11-04},
	date = {2019-11-28},
	eprinttype = {arxiv},
	eprint = {1911.09983 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/26U8ZIN8/Sun et al. - 2019 - TreeGen A Tree-Based Transformer Architecture for.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/ZJAMA2EB/1911.html:text/html},
}

@online{sun_treegen_2022-1,
	title = {{TreeGen}},
	rights = {{MIT}},
	url = {https://github.com/zysszy/TreeGen},
	abstract = {A Tree-Based Transformer Architecture for Code Generation. ({AAAI}'20)},
	author = {Sun, Zeyu},
	urldate = {2022-11-04},
	date = {2022-10-15},
	note = {original-date: 2019-11-20T05:52:50Z},
}

@inproceedings{wang_unified_2022,
	title = {Unified Abstract Syntax Tree Representation Learning for Cross-Language Program Classification},
	url = {http://arxiv.org/abs/2205.00424},
	doi = {10.1145/3524610.3527915},
	abstract = {Program classification can be regarded as a high-level abstraction of code, laying a foundation for various tasks related to source code comprehension, and has a very wide range of applications in the field of software engineering, such as code clone detection, code smell classification, defects classification, etc. The cross-language program classification can realize code transfer in different programming languages, and can also promote cross-language code reuse, thereby helping developers to write code quickly and reduce the development time of code transfer. Most of the existing studies focus on the semantic learning of the code, whilst few studies are devoted to cross-language tasks. The main challenge of cross-language program classification is how to extract semantic features of different programming languages. In order to cope with this difficulty, we propose a Unified Abstract Syntax Tree (namely {UAST} in this paper) neural network. In detail, the core idea of {UAST} consists of two unified mechanisms. First, {UAST} learns an {AST} representation by unifying the {AST} traversal sequence and graph-like {AST} structure for capturing semantic code features. Second, we construct a mechanism called unified vocabulary, which can reduce the feature gap between different programming languages, so it can achieve the role of cross-language program classification. Besides, we collect a dataset containing 20,000 files of five programming languages, which can be used as a benchmark dataset for the cross-language program classification task. We have done experiments on two datasets, and the results show that our proposed approach outperforms the state-of-the-art baselines in terms of four evaluation metrics (Precision, Recall, F1-score, and Accuracy).},
	pages = {390--400},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} International Conference on Program Comprehension},
	author = {Wang, Kesu and Yan, Meng and Zhang, He and Hu, Haibo},
	urldate = {2022-11-04},
	date = {2022-05-16},
	eprinttype = {arxiv},
	eprint = {2205.00424 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/HU46SND4/Wang et al. - 2022 - Unified Abstract Syntax Tree Representation Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/6E2SQH2M/2205.html:text/html},
}

@misc{kim_pure_2022-1,
	title = {Pure Transformers are Powerful Graph Learners},
	url = {http://arxiv.org/abs/2207.02505},
	doi = {10.48550/arXiv.2207.02505},
	abstract = {We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-{IGN}) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks ({GNN}). When trained on a large-scale graph dataset ({PCQM}4Mv2), our method coined Tokenized Graph Transformer ({TokenGT}) achieves significantly better results compared to {GNN} baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.},
	number = {{arXiv}:2207.02505},
	publisher = {{arXiv}},
	author = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
	urldate = {2022-11-04},
	date = {2022-10-22},
	eprinttype = {arxiv},
	eprint = {2207.02505 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/germanarutunov/Zotero/storage/NYRT8ZSU/Kim et al. - 2022 - Pure Transformers are Powerful Graph Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/germanarutunov/Zotero/storage/V6IA69P6/2207.html:text/html},
}

@misc{choromanski_rethinking_2020,
	title = {Rethinking Attention with Performers},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	date = {2020},
	note = {\_eprint: 2009.14794},
}

@article{arutyunov_big_2022,
	title = {Big Transformers for Code Generation},
	volume = {34},
	url = {https://doi.org/10.15514%2Fispras-2022-34%284%29-6},
	doi = {10.15514/ispras-2022-34(4)-6},
	pages = {79--88},
	number = {4},
	journaltitle = {Proceedings of the Institute for System Programming of the {RAS}},
	author = {Arutyunov, German Arsenovich and Avdoshin, Sergey Mikchailovitch},
	date = {2022},
	note = {Publisher: Institute for System Programming of the Russian Academy of Sciences},
}
